{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, Lambda\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=95000\n",
    "maxlen=70\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preparation(predict=False):\n",
    "    start_time = time.time()\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "\n",
    "    \n",
    "    if predict:\n",
    "        test_df = pd.read_csv(\"test.csv\")\n",
    "        print(\"Test shape : \",test_df.shape)\n",
    "        test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "        \n",
    "        \n",
    "    ## split to train and val\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.08, random_state=2018)\n",
    "    \n",
    "    # Bootstrap\n",
    "    #count_class_0, count_class_1 = train_df['target'].value_counts()\n",
    "    #df_class_0 = train_df[train_df['target'] == 0]\n",
    "    #df_class_1 = train_df[train_df['target'] == 1]\n",
    "    #df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "    #train_df = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "    #print(\"Train shape after Bootstrap : \",train_df.shape)\n",
    "    #target_counts = train_df.target.value_counts()\n",
    "    #print('sincere questions: {}%'. format(((target_counts[0]*100)/train_df.shape[0]).round(2)))\n",
    "    #print('insincere questions: {}%'. format(((target_counts[1]*100)/train_df.shape[0]).round(2)))\n",
    "\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].values\n",
    "    val_X = val_df[\"question_text\"].values\n",
    "\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    print('fitting text to tokenizer..')\n",
    "    check_point1 = time.time()\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    check_point2 = time.time()\n",
    "    \n",
    "    print('fitting took {:.2f} seconds to finish'.format(check_point2 - check_point1))\n",
    "    \n",
    "    print('transforming text to sequence of word indices..')\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    check_point3 = time.time()\n",
    "    print('transforming took {:.2f} seconds to finish'.format(check_point3 - check_point2))\n",
    "    if predict:\n",
    "        test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    print('padding sentence to the same length..')\n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "    check_point4 = time.time()\n",
    "    print('padding took {:.2f} seconds to finish'.format(check_point4 - check_point3))\n",
    "    \n",
    "    if predict:\n",
    "        test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "        \n",
    "    print('it took {:.2f} seconds to finish data prepartation'.format(time.time() - start_time))\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    val_y = val_df['target'].values    \n",
    "    \n",
    "    if predict:\n",
    "        return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index, tokenizer\n",
    "    else:\n",
    "        return train_X, val_X, train_y, val_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n",
      "fitting text to tokenizer..\n",
      "Found 427454 unique tokens.\n",
      "fitting took 17.92 seconds to finish\n",
      "transforming text to sequence of word indices..\n",
      "transforming took 20.00 seconds to finish\n",
      "padding sentence to the same length..\n",
      "padding took 6.13 seconds to finish\n",
      "it took 47.77 seconds to finish data prepartation\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, test_X, train_y, val_y, word_index, tokenizer = data_preparation(predict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki News FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fasttext(word_index):\n",
    "    EMBEDDING_FILE = 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c431e2991b6425a8dbdb569ac03573f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix1 = load_glove(word_index)\n",
    "#embedding_fname='glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c40a71749a94c4d86c8cfc564b933b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix2 = load_fasttext(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f4c98f4fb2491c82d3a1d34f7eb916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix3 = load_para(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.mean([embedding_matrix1,embedding_matrix2, embedding_matrix3], axis = 0)\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(embedding_matrix):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
    "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn_flip(embedding_matrix):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Lambda(lambda x: K.reverse(x,axes=-1))(inp)\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(x)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
    "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_lstm_atten(embedding_matrix):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gru_srk_atten(embedding_matrix):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    x = Attention(maxlen)(x) \n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_lstm_du(embedding_matrix):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(64, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gru_atten_3(embedding_matrix):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Predict and Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pred(model, epochs=2):\n",
    "    for e in range(epochs):\n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n",
    "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "\n",
    "        best_thresh = 0.5\n",
    "        best_score = 0.0\n",
    "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "            thresh = np.round(thresh, 2)\n",
    "            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "            if score > best_score:\n",
    "                best_thresh = thresh\n",
    "                best_score = score\n",
    "\n",
    "        print(\"Val F1 Score: {:.4f}\".format(best_score))\n",
    "\n",
    "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
    "    return pred_val_y, pred_test_y, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3134s 3ms/step - loss: 0.1186 - acc: 0.9539 - val_loss: 0.1077 - val_acc: 0.9578\n",
      "Val F1 Score: 0.6519\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3374s 3ms/step - loss: 0.0929 - acc: 0.9630 - val_loss: 0.1108 - val_acc: 0.9566\n",
      "Val F1 Score: 0.6470\n"
     ]
    }
   ],
   "source": [
    "model1 = model_cnn(embedding_matrix)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model1, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.save('model1.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3285s 3ms/step - loss: 0.1177 - acc: 0.9543 - val_loss: 0.1085 - val_acc: 0.9569\n",
      "Val F1 Score: 0.6515\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3447s 3ms/step - loss: 0.0926 - acc: 0.9631 - val_loss: 0.1125 - val_acc: 0.9560\n",
      "Val F1 Score: 0.6501\n"
     ]
    }
   ],
   "source": [
    "model2 = model_cnn_flip(embedding_matrix)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model2, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'CNN flip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save('model2.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 8505s 7ms/step - loss: 0.1189 - acc: 0.9541 - val_loss: 0.1069 - val_acc: 0.9575\n",
      "Val F1 Score: 0.6545\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 8491s 7ms/step - loss: 0.0936 - acc: 0.9625 - val_loss: 0.1075 - val_acc: 0.9580\n",
      "Val F1 Score: 0.6617\n"
     ]
    }
   ],
   "source": [
    "model3 = model_lstm_atten(embedding_matrix)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model3, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'LSTM atten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.save('model3.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3528s 3ms/step - loss: 0.1205 - acc: 0.9541 - val_loss: 0.1069 - val_acc: 0.9577\n",
      "Val F1 Score: 0.6550\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3523s 3ms/step - loss: 0.0934 - acc: 0.9626 - val_loss: 0.1083 - val_acc: 0.9583\n",
      "Val F1 Score: 0.6579\n"
     ]
    }
   ],
   "source": [
    "model4 = model_gru_srk_atten(embedding_matrix)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model4, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'GRU srk atten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4.save('model4.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3708s 3ms/step - loss: 0.1173 - acc: 0.9547 - val_loss: 0.1056 - val_acc: 0.9580\n",
      "Val F1 Score: 0.6560\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 7448s 6ms/step - loss: 0.0918 - acc: 0.9632 - val_loss: 0.1055 - val_acc: 0.9581\n",
      "Val F1 Score: 0.6603\n"
     ]
    }
   ],
   "source": [
    "model5 = model_lstm_du(embedding_matrix)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model5, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'LSTM du'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5.save('model5.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 9501s 8ms/step - loss: 0.1254 - acc: 0.9524 - val_loss: 0.1140 - val_acc: 0.9550\n",
      "Val F1 Score: 0.6336\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 11427s 10ms/step - loss: 0.1101 - acc: 0.9572 - val_loss: 0.1102 - val_acc: 0.9567\n",
      "Val F1 Score: 0.6504\n"
     ]
    }
   ],
   "source": [
    "model6 = model_gru_atten_3(embedding_matrix)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model6, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'GRU atten 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.save('model6.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7 = model_lstm_atten(embedding_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 8792s 7ms/step - loss: 0.1168 - acc: 0.9546 - val_loss: 0.1081 - val_acc: 0.9575\n",
      "Val F1 Score: 0.6598\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 8435s 7ms/step - loss: 0.0939 - acc: 0.9626 - val_loss: 0.1094 - val_acc: 0.9581\n",
      "Val F1 Score: 0.6652\n"
     ]
    }
   ],
   "source": [
    "pred_val_y, pred_test_y, best_score = train_pred(model7, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs.append([pred_val_y, pred_test_y, best_score, 'LSTM atten glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.save('model7.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3950s 3ms/step - loss: 0.1221 - acc: 0.9539 - val_loss: 0.1076 - val_acc: 0.9570\n",
      "Val F1 Score: 0.6558\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3862s 3ms/step - loss: 0.0965 - acc: 0.9621 - val_loss: 0.1071 - val_acc: 0.9581\n",
      "Val F1 Score: 0.6613\n"
     ]
    }
   ],
   "source": [
    "model8 = model_gru_srk_atten(embedding_matrix1)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model8, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'GRU srk atten glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model8.save('model8.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 4677s 4ms/step - loss: 0.1182 - acc: 0.9541 - val_loss: 0.1066 - val_acc: 0.9575\n",
      "Val F1 Score: 0.6557\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 4143s 3ms/step - loss: 0.0941 - acc: 0.9624 - val_loss: 0.1065 - val_acc: 0.9577\n",
      "Val F1 Score: 0.6633\n"
     ]
    }
   ],
   "source": [
    "model9 = model_lstm_du(embedding_matrix1)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model9, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'LSTM du glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model9.save('model9.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3154s 3ms/step - loss: 0.1246 - acc: 0.9524 - val_loss: 0.1093 - val_acc: 0.9566\n",
      "Val F1 Score: 0.6458\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3198s 3ms/step - loss: 0.0961 - acc: 0.9620 - val_loss: 0.1098 - val_acc: 0.9573\n",
      "Val F1 Score: 0.6491\n"
     ]
    }
   ],
   "source": [
    "model10 = model_cnn(embedding_matrix1)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model10, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs.append([pred_val_y, pred_test_y, best_score, 'CNN glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model10.save('CNN_glove.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3118s 3ms/step - loss: 0.1209 - acc: 0.9533 - val_loss: 0.1093 - val_acc: 0.9567\n",
      "Val F1 Score: 0.6477\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 3257s 3ms/step - loss: 0.0959 - acc: 0.9619 - val_loss: 0.1100 - val_acc: 0.9571\n",
      "Val F1 Score: 0.6498\n"
     ]
    }
   ],
   "source": [
    "model11 = model_cnn_flip(embedding_matrix1)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model11, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'CNN flip glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model11.save('CNN_flip_glove.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 9940s 8ms/step - loss: 0.1231 - acc: 0.9531 - val_loss: 0.1122 - val_acc: 0.9566\n",
      "Val F1 Score: 0.6435\n",
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/1\n",
      "1201632/1201632 [==============================] - 8570s 7ms/step - loss: 0.1065 - acc: 0.9582 - val_loss: 0.1077 - val_acc: 0.9571\n",
      "Val F1 Score: 0.6550\n"
     ]
    }
   ],
   "source": [
    "model12 = model_gru_atten_3(embedding_matrix1)\n",
    "pred_val_y, pred_test_y, best_score = train_pred(model12, epochs = 2)\n",
    "outputs.append([pred_val_y, pred_test_y, best_score, 'GRU atten 3 glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model12.save('GRU_3_glove.h5', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470461560548418 CNN\n",
      "0.6490960575038117 CNN glove\n",
      "0.6498450641876937 CNN flip glove\n",
      "0.6501303643154112 CNN flip\n",
      "0.6510047281323877 GRU atten 3\n",
      "0.6549674092113745 GRU atten 3 glove\n",
      "0.6580165521110956 LSTM atten\n",
      "0.6593295928643361 LSTM du\n",
      "0.6604736916749399 GRU srk atten\n",
      "0.6613148188327186 GRU srk atten glove\n",
      "0.6632911392405064 LSTM du glove\n",
      "0.6652069501341998 LSTM atten glove\n"
     ]
    }
   ],
   "source": [
    "outputs.sort(key=lambda x: x[2])\n",
    "for output in outputs:\n",
    "    print(output[2], output[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in outputs:\n",
    "    output.append(i)\n",
    "del(output[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.6100257621296694\n",
      "F1 score at threshold 0.11 is 0.6184557853057633\n",
      "F1 score at threshold 0.12 is 0.6260341043390174\n",
      "F1 score at threshold 0.13 is 0.6326366559485531\n",
      "F1 score at threshold 0.14 is 0.6377082724349605\n",
      "F1 score at threshold 0.15 is 0.6430738119312437\n",
      "F1 score at threshold 0.16 is 0.648207484432622\n",
      "F1 score at threshold 0.17 is 0.6518273240127546\n",
      "F1 score at threshold 0.18 is 0.6552475864216755\n",
      "F1 score at threshold 0.19 is 0.6592143488695213\n",
      "F1 score at threshold 0.2 is 0.661632\n",
      "F1 score at threshold 0.21 is 0.6625607779578606\n",
      "F1 score at threshold 0.22 is 0.6638655462184875\n",
      "F1 score at threshold 0.23 is 0.6670208527028821\n",
      "F1 score at threshold 0.24 is 0.6702913142703719\n",
      "F1 score at threshold 0.25 is 0.6723271294661051\n",
      "F1 score at threshold 0.26 is 0.6726186388297507\n",
      "F1 score at threshold 0.27 is 0.6738057158751131\n",
      "F1 score at threshold 0.28 is 0.6748207507380852\n",
      "F1 score at threshold 0.29 is 0.6754373488835158\n",
      "F1 score at threshold 0.3 is 0.6770181870462224\n",
      "F1 score at threshold 0.31 is 0.6768672951414068\n",
      "F1 score at threshold 0.32 is 0.6772161172161172\n",
      "F1 score at threshold 0.33 is 0.6760875998816218\n",
      "F1 score at threshold 0.34 is 0.6744290192566055\n",
      "F1 score at threshold 0.35 is 0.6734524617356555\n",
      "F1 score at threshold 0.36 is 0.6712818562190948\n",
      "F1 score at threshold 0.37 is 0.6686149350149966\n",
      "F1 score at threshold 0.38 is 0.6672362901972969\n",
      "F1 score at threshold 0.39 is 0.6682341876322595\n",
      "F1 score at threshold 0.4 is 0.6656638429261341\n",
      "F1 score at threshold 0.41 is 0.6637323943661971\n",
      "F1 score at threshold 0.42 is 0.6607979324826361\n",
      "F1 score at threshold 0.43 is 0.6567651379141505\n",
      "F1 score at threshold 0.44 is 0.6545754328112119\n",
      "F1 score at threshold 0.45 is 0.6496380730510026\n",
      "F1 score at threshold 0.46 is 0.6479463537300924\n",
      "F1 score at threshold 0.47 is 0.6461330174310375\n",
      "F1 score at threshold 0.48 is 0.6427961032302171\n",
      "F1 score at threshold 0.49 is 0.6372185316193598\n",
      "F1 score at threshold 0.5 is 0.6325159263461035\n",
      "Best threshold:  0.32\n"
     ]
    }
   ],
   "source": [
    "pred_val_y = np.mean([output[i][0] for i in range(len(output))], axis = 0)\n",
    "\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test_y = np.mean([outputs[i][1] for i in range(len(outputs))], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
