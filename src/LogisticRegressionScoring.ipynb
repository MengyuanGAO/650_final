{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1306122.0\n",
       "mean          70.7\n",
       "std           38.8\n",
       "min            1.0\n",
       "25%           45.0\n",
       "50%           60.0\n",
       "75%           85.0\n",
       "max         1017.0\n",
       "Name: question_text, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.question_text.str.len().describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    56370.0\n",
       "mean        70.5\n",
       "std         38.7\n",
       "min         11.0\n",
       "25%         45.0\n",
       "50%         60.0\n",
       "75%         85.0\n",
       "max        588.0\n",
       "Name: question_text, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.question_text.str.len().describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    187558\n",
       "1     32373\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.question_text.str.len() > 100].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['char_length'] = train_df.question_text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16        What is fapping?\n",
       "632    What is NuAge Skin?\n",
       "732          Is UPSE good?\n",
       "907        How can I post?\n",
       "973    Who created idioms?\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.question_text.str.len() < 20].question_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = train_df[train_df.target == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why are israelis destroying churches/mosques in Israel? Where is the outrage?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.sample(1).iloc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>char_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0000e91571b60c2fb487</td>\n",
       "      <td>Has the United States become the largest dicta...</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>00013ceca3f624b09f42</td>\n",
       "      <td>Which babies are more sweeter to their parents...</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0004a7fcb2bf73076489</td>\n",
       "      <td>If blacks support school choice and mandatory ...</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>00052793eaa287aff1e1</td>\n",
       "      <td>I am gay boy and I love my cousin (boy). He is...</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>000537213b01fd77b58a</td>\n",
       "      <td>Which races have the smallest penis?</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      qid                                      question_text  \\\n",
       "22   0000e91571b60c2fb487  Has the United States become the largest dicta...   \n",
       "30   00013ceca3f624b09f42  Which babies are more sweeter to their parents...   \n",
       "110  0004a7fcb2bf73076489  If blacks support school choice and mandatory ...   \n",
       "114  00052793eaa287aff1e1  I am gay boy and I love my cousin (boy). He is...   \n",
       "115  000537213b01fd77b58a               Which races have the smallest penis?   \n",
       "\n",
       "     target  char_length  \n",
       "22        1           67  \n",
       "30        1           86  \n",
       "110       1          102  \n",
       "114       1          137  \n",
       "115       1           36  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid                                           000485e6dd4b149fe051\n",
       "question_text    Can I start freelancing after finishing Udacit...\n",
       "target                                                           0\n",
       "char_length                                                     75\n",
       "Name: 107, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>char_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0012d9af133219e1b370</td>\n",
       "      <td>What is the meaning of relationship with a gir...</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0028f67ccff988aee531</td>\n",
       "      <td>What is the meaning of nudge in hike?</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>004299fc61797c2c9895</td>\n",
       "      <td>What is the meaning of life according to Chris...</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>006192347b2ba501281b</td>\n",
       "      <td>What is the meaning of the word \"Analhak\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>00b30616acc5be7a2b19</td>\n",
       "      <td>What is the meaning of \"ask of\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       qid                                      question_text  \\\n",
       "401   0012d9af133219e1b370  What is the meaning of relationship with a gir...   \n",
       "853   0028f67ccff988aee531              What is the meaning of nudge in hike?   \n",
       "1341  004299fc61797c2c9895  What is the meaning of life according to Chris...   \n",
       "1958  006192347b2ba501281b         What is the meaning of the word \"Analhak\"?   \n",
       "3586  00b30616acc5be7a2b19                   What is the meaning of \"ask of\"?   \n",
       "\n",
       "      target  char_length  \n",
       "401        0           62  \n",
       "853        0           37  \n",
       "1341       0           74  \n",
       "1958       0           42  \n",
       "3586       0           32  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.question_text.str.startswith('What is the meaning of')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=95000\n",
    "maxlen=70\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preparation(predict=False, sub_train=True):\n",
    "    start_time = time.time()\n",
    "    train_df = pd.read_csv(\"./train.csv\")\n",
    "    if sub_train:\n",
    "        train_df = train_df.sample(frac=0.3)\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    if predict:\n",
    "        test_df = pd.read_csv(\"./test.csv\")\n",
    "        print(\"Test shape : \",test_df.shape)\n",
    "        test_X = test_df[\"question_text\"].fillna(\"_##_\").values    \n",
    "    ## split to train and val\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.08, random_state=2018)\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].values\n",
    "    val_X = val_df[\"question_text\"].values\n",
    "\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    print('fitting text to tokenizer..')\n",
    "    check_point1 = time.time()\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    check_point2 = time.time()\n",
    "    print('fitting took {:.2f} seconds to finish'.format(check_point2 - check_point1))\n",
    "#     save_text_tokenizer(tokenizer, \"tokenizer\")\n",
    "    \n",
    "    print('transforming text to sequence of word indices..')\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    check_point3 = time.time()\n",
    "    print('transforming took {:.2f} seconds to finish'.format(check_point3 - check_point2))\n",
    "    if predict:\n",
    "        test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    print('padding sentence to the same length..')\n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "    check_point4 = time.time()\n",
    "    print('padding took {:.2f} seconds to finish'.format(check_point4 - check_point3))\n",
    "    \n",
    "    if predict:\n",
    "        test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "        \n",
    "    print('it took {:.2f} seconds to finish data prepartation'.format(time.time() - start_time))\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    val_y = val_df['target'].values  \n",
    "    \n",
    "#     trn_idx = np.random.permutation(len(train_X))\n",
    "#     val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "#     train_X = train_X[trn_idx]\n",
    "#     val_X = val_X[val_idx]\n",
    "#     train_y = train_y[trn_idx]\n",
    "#     val_y = val_y[val_idx]    \n",
    "    \n",
    "    if predict:\n",
    "        return train_X, val_X, test_X, train_y, val_y, tokenizer.word_index, tokenizer, val_df\n",
    "    else:\n",
    "        return train_X, val_X, train_y, val_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (391837, 3)\n",
      "Test shape :  (56370, 2)\n",
      "fitting text to tokenizer..\n",
      "Found 202140 unique tokens.\n",
      "fitting took 9.27 seconds to finish\n",
      "transforming text to sequence of word indices..\n",
      "transforming took 8.00 seconds to finish\n",
      "padding sentence to the same length..\n",
      "padding took 4.37 seconds to finish\n",
      "it took 27.01 seconds to finish data prepartation\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, test_X, train_y, val_y, word_index, tokenizer, val_df = data_preparation(predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index, embedding_fname='glove.840B.300d.txt'):\n",
    "    EMBEDDING_FILE = './glove.840B.300d/' + embedding_fname\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e58af4481f4a2487836a7534e01980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix1 = load_glove(word_index)\n",
    "# , embedding_fname='glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, Lambda\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(embedding_matrix):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
    "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360490 samples, validate on 31347 samples\n",
      "Epoch 1/1\n",
      "103936/360490 [=======>......................] - ETA: 40:39 - loss: 0.8949 - acc: 0.25 - ETA: 30:29 - loss: 0.6443 - acc: 0.58 - ETA: 27:12 - loss: 0.4950 - acc: 0.70 - ETA: 25:34 - loss: 0.4159 - acc: 0.76 - ETA: 24:43 - loss: 0.3804 - acc: 0.80 - ETA: 24:03 - loss: 0.3783 - acc: 0.82 - ETA: 23:40 - loss: 0.3756 - acc: 0.84 - ETA: 22:51 - loss: 0.3837 - acc: 0.85 - ETA: 21:51 - loss: 0.3694 - acc: 0.86 - ETA: 21:40 - loss: 0.3691 - acc: 0.86 - ETA: 21:32 - loss: 0.3669 - acc: 0.87 - ETA: 21:29 - loss: 0.3636 - acc: 0.88 - ETA: 21:30 - loss: 0.3565 - acc: 0.88 - ETA: 21:25 - loss: 0.3502 - acc: 0.88 - ETA: 21:16 - loss: 0.3493 - acc: 0.89 - ETA: 21:09 - loss: 0.3400 - acc: 0.89 - ETA: 21:08 - loss: 0.3326 - acc: 0.89 - ETA: 21:06 - loss: 0.3240 - acc: 0.89 - ETA: 21:00 - loss: 0.3185 - acc: 0.90 - ETA: 20:56 - loss: 0.3122 - acc: 0.90 - ETA: 20:52 - loss: 0.3061 - acc: 0.90 - ETA: 20:49 - loss: 0.2998 - acc: 0.90 - ETA: 20:47 - loss: 0.2951 - acc: 0.90 - ETA: 20:43 - loss: 0.2914 - acc: 0.90 - ETA: 20:41 - loss: 0.2871 - acc: 0.91 - ETA: 20:38 - loss: 0.2837 - acc: 0.91 - ETA: 20:35 - loss: 0.2811 - acc: 0.91 - ETA: 20:33 - loss: 0.2776 - acc: 0.91 - ETA: 20:23 - loss: 0.2728 - acc: 0.91 - ETA: 20:10 - loss: 0.2692 - acc: 0.91 - ETA: 20:03 - loss: 0.2659 - acc: 0.91 - ETA: 19:57 - loss: 0.2634 - acc: 0.91 - ETA: 19:54 - loss: 0.2597 - acc: 0.91 - ETA: 19:49 - loss: 0.2579 - acc: 0.91 - ETA: 19:45 - loss: 0.2569 - acc: 0.91 - ETA: 19:39 - loss: 0.2550 - acc: 0.91 - ETA: 19:35 - loss: 0.2539 - acc: 0.91 - ETA: 19:31 - loss: 0.2511 - acc: 0.91 - ETA: 19:28 - loss: 0.2481 - acc: 0.92 - ETA: 19:24 - loss: 0.2461 - acc: 0.92 - ETA: 19:19 - loss: 0.2442 - acc: 0.92 - ETA: 19:15 - loss: 0.2434 - acc: 0.92 - ETA: 19:07 - loss: 0.2410 - acc: 0.92 - ETA: 19:01 - loss: 0.2403 - acc: 0.92 - ETA: 18:55 - loss: 0.2381 - acc: 0.92 - ETA: 18:50 - loss: 0.2366 - acc: 0.92 - ETA: 18:50 - loss: 0.2358 - acc: 0.92 - ETA: 18:49 - loss: 0.2345 - acc: 0.92 - ETA: 18:48 - loss: 0.2332 - acc: 0.92 - ETA: 18:47 - loss: 0.2322 - acc: 0.92 - ETA: 18:41 - loss: 0.2303 - acc: 0.92 - ETA: 18:39 - loss: 0.2283 - acc: 0.92 - ETA: 18:35 - loss: 0.2269 - acc: 0.92 - ETA: 18:32 - loss: 0.2252 - acc: 0.92 - ETA: 18:28 - loss: 0.2240 - acc: 0.92 - ETA: 18:24 - loss: 0.2229 - acc: 0.92 - ETA: 18:19 - loss: 0.2210 - acc: 0.92 - ETA: 18:16 - loss: 0.2199 - acc: 0.92 - ETA: 18:09 - loss: 0.2190 - acc: 0.92 - ETA: 18:04 - loss: 0.2184 - acc: 0.92 - ETA: 17:58 - loss: 0.2176 - acc: 0.92 - ETA: 17:53 - loss: 0.2163 - acc: 0.92 - ETA: 17:49 - loss: 0.2155 - acc: 0.92 - ETA: 17:45 - loss: 0.2150 - acc: 0.92 - ETA: 17:40 - loss: 0.2147 - acc: 0.92 - ETA: 17:35 - loss: 0.2139 - acc: 0.92 - ETA: 17:30 - loss: 0.2130 - acc: 0.92 - ETA: 17:29 - loss: 0.2115 - acc: 0.92 - ETA: 17:28 - loss: 0.2110 - acc: 0.92 - ETA: 17:27 - loss: 0.2101 - acc: 0.92 - ETA: 17:28 - loss: 0.2090 - acc: 0.92 - ETA: 17:28 - loss: 0.2081 - acc: 0.92 - ETA: 17:27 - loss: 0.2068 - acc: 0.93 - ETA: 17:27 - loss: 0.2061 - acc: 0.93 - ETA: 17:27 - loss: 0.2053 - acc: 0.93 - ETA: 17:26 - loss: 0.2045 - acc: 0.93 - ETA: 17:26 - loss: 0.2036 - acc: 0.93 - ETA: 17:26 - loss: 0.2029 - acc: 0.93 - ETA: 17:25 - loss: 0.2024 - acc: 0.93 - ETA: 17:25 - loss: 0.2016 - acc: 0.93 - ETA: 17:25 - loss: 0.2016 - acc: 0.93 - ETA: 17:24 - loss: 0.2008 - acc: 0.93 - ETA: 17:22 - loss: 0.1998 - acc: 0.93 - ETA: 17:21 - loss: 0.1999 - acc: 0.93 - ETA: 17:21 - loss: 0.1990 - acc: 0.93 - ETA: 17:19 - loss: 0.1983 - acc: 0.93 - ETA: 17:19 - loss: 0.1978 - acc: 0.93 - ETA: 17:18 - loss: 0.1969 - acc: 0.93 - ETA: 17:17 - loss: 0.1959 - acc: 0.93 - ETA: 17:16 - loss: 0.1958 - acc: 0.93 - ETA: 17:15 - loss: 0.1956 - acc: 0.93 - ETA: 17:14 - loss: 0.1952 - acc: 0.93 - ETA: 17:14 - loss: 0.1948 - acc: 0.93 - ETA: 17:14 - loss: 0.1945 - acc: 0.93 - ETA: 17:12 - loss: 0.1938 - acc: 0.93 - ETA: 17:13 - loss: 0.1936 - acc: 0.93 - ETA: 17:11 - loss: 0.1931 - acc: 0.93 - ETA: 17:10 - loss: 0.1926 - acc: 0.93 - ETA: 17:09 - loss: 0.1922 - acc: 0.93 - ETA: 17:08 - loss: 0.1914 - acc: 0.93 - ETA: 17:07 - loss: 0.1908 - acc: 0.93 - ETA: 17:05 - loss: 0.1908 - acc: 0.93 - ETA: 17:04 - loss: 0.1905 - acc: 0.93 - ETA: 17:02 - loss: 0.1900 - acc: 0.93 - ETA: 17:00 - loss: 0.1896 - acc: 0.93 - ETA: 16:58 - loss: 0.1891 - acc: 0.93 - ETA: 16:56 - loss: 0.1893 - acc: 0.93 - ETA: 16:55 - loss: 0.1889 - acc: 0.93 - ETA: 16:53 - loss: 0.1887 - acc: 0.93 - ETA: 16:51 - loss: 0.1885 - acc: 0.93 - ETA: 16:50 - loss: 0.1879 - acc: 0.93 - ETA: 16:47 - loss: 0.1875 - acc: 0.93 - ETA: 16:45 - loss: 0.1874 - acc: 0.93 - ETA: 16:44 - loss: 0.1867 - acc: 0.93 - ETA: 16:43 - loss: 0.1859 - acc: 0.93 - ETA: 16:41 - loss: 0.1855 - acc: 0.93 - ETA: 16:39 - loss: 0.1848 - acc: 0.93 - ETA: 16:38 - loss: 0.1842 - acc: 0.93 - ETA: 16:36 - loss: 0.1838 - acc: 0.93 - ETA: 16:35 - loss: 0.1836 - acc: 0.93 - ETA: 16:33 - loss: 0.1832 - acc: 0.93 - ETA: 16:32 - loss: 0.1829 - acc: 0.93 - ETA: 16:29 - loss: 0.1828 - acc: 0.93 - ETA: 16:26 - loss: 0.1829 - acc: 0.93 - ETA: 16:24 - loss: 0.1823 - acc: 0.93 - ETA: 16:22 - loss: 0.1822 - acc: 0.93 - ETA: 16:21 - loss: 0.1817 - acc: 0.93 - ETA: 16:20 - loss: 0.1813 - acc: 0.93 - ETA: 16:17 - loss: 0.1811 - acc: 0.93 - ETA: 16:14 - loss: 0.1808 - acc: 0.93 - ETA: 16:12 - loss: 0.1804 - acc: 0.93 - ETA: 16:10 - loss: 0.1801 - acc: 0.93 - ETA: 16:08 - loss: 0.1797 - acc: 0.93 - ETA: 16:05 - loss: 0.1791 - acc: 0.93 - ETA: 16:02 - loss: 0.1789 - acc: 0.93 - ETA: 15:59 - loss: 0.1783 - acc: 0.93 - ETA: 15:58 - loss: 0.1782 - acc: 0.93 - ETA: 15:57 - loss: 0.1778 - acc: 0.93 - ETA: 15:55 - loss: 0.1774 - acc: 0.93 - ETA: 15:52 - loss: 0.1769 - acc: 0.93 - ETA: 15:50 - loss: 0.1765 - acc: 0.93 - ETA: 15:49 - loss: 0.1760 - acc: 0.93 - ETA: 15:47 - loss: 0.1758 - acc: 0.93 - ETA: 15:45 - loss: 0.1753 - acc: 0.93 - ETA: 15:43 - loss: 0.1751 - acc: 0.93 - ETA: 15:40 - loss: 0.1747 - acc: 0.93 - ETA: 15:38 - loss: 0.1743 - acc: 0.93 - ETA: 15:36 - loss: 0.1739 - acc: 0.93 - ETA: 15:34 - loss: 0.1735 - acc: 0.93 - ETA: 15:32 - loss: 0.1732 - acc: 0.93 - ETA: 15:30 - loss: 0.1731 - acc: 0.93 - ETA: 15:29 - loss: 0.1728 - acc: 0.93 - ETA: 15:27 - loss: 0.1726 - acc: 0.93 - ETA: 15:26 - loss: 0.1724 - acc: 0.93 - ETA: 15:25 - loss: 0.1723 - acc: 0.93 - ETA: 15:24 - loss: 0.1719 - acc: 0.93 - ETA: 15:23 - loss: 0.1717 - acc: 0.93 - ETA: 15:22 - loss: 0.1713 - acc: 0.93 - ETA: 15:20 - loss: 0.1711 - acc: 0.93 - ETA: 15:19 - loss: 0.1710 - acc: 0.93 - ETA: 15:17 - loss: 0.1708 - acc: 0.93 - ETA: 15:14 - loss: 0.1706 - acc: 0.93 - ETA: 15:12 - loss: 0.1704 - acc: 0.93 - ETA: 15:10 - loss: 0.1700 - acc: 0.93 - ETA: 15:09 - loss: 0.1695 - acc: 0.93 - ETA: 15:07 - loss: 0.1692 - acc: 0.94 - ETA: 15:05 - loss: 0.1689 - acc: 0.94 - ETA: 15:03 - loss: 0.1688 - acc: 0.94 - ETA: 15:02 - loss: 0.1685 - acc: 0.94 - ETA: 15:00 - loss: 0.1685 - acc: 0.94 - ETA: 14:58 - loss: 0.1682 - acc: 0.94 - ETA: 14:57 - loss: 0.1681 - acc: 0.94 - ETA: 14:55 - loss: 0.1679 - acc: 0.94 - ETA: 14:54 - loss: 0.1678 - acc: 0.94 - ETA: 14:53 - loss: 0.1677 - acc: 0.94 - ETA: 14:52 - loss: 0.1677 - acc: 0.94 - ETA: 14:50 - loss: 0.1673 - acc: 0.94 - ETA: 14:49 - loss: 0.1671 - acc: 0.94 - ETA: 14:47 - loss: 0.1669 - acc: 0.94 - ETA: 14:46 - loss: 0.1666 - acc: 0.94 - ETA: 14:45 - loss: 0.1668 - acc: 0.94 - ETA: 14:43 - loss: 0.1667 - acc: 0.94 - ETA: 14:42 - loss: 0.1666 - acc: 0.94 - ETA: 14:41 - loss: 0.1663 - acc: 0.94 - ETA: 14:39 - loss: 0.1660 - acc: 0.94 - ETA: 14:38 - loss: 0.1659 - acc: 0.94 - ETA: 14:36 - loss: 0.1656 - acc: 0.94 - ETA: 14:35 - loss: 0.1654 - acc: 0.94 - ETA: 14:34 - loss: 0.1652 - acc: 0.94 - ETA: 14:32 - loss: 0.1649 - acc: 0.94 - ETA: 14:31 - loss: 0.1647 - acc: 0.94 - ETA: 14:30 - loss: 0.1645 - acc: 0.94 - ETA: 14:28 - loss: 0.1642 - acc: 0.94 - ETA: 14:27 - loss: 0.1643 - acc: 0.94 - ETA: 14:25 - loss: 0.1642 - acc: 0.94 - ETA: 14:24 - loss: 0.1640 - acc: 0.94 - ETA: 14:22 - loss: 0.1640 - acc: 0.94 - ETA: 14:21 - loss: 0.1639 - acc: 0.94 - ETA: 14:19 - loss: 0.1636 - acc: 0.94 - ETA: 14:18 - loss: 0.1635 - acc: 0.94 - ETA: 14:17 - loss: 0.1635 - acc: 0.94 - ETA: 14:15 - loss: 0.1633 - acc: 0.94 - ETA: 14:13 - loss: 0.1631 - acc: 0.94208384/360490 [================>.............] - ETA: 14:12 - loss: 0.1628 - acc: 0.94 - ETA: 14:11 - loss: 0.1628 - acc: 0.94 - ETA: 14:09 - loss: 0.1626 - acc: 0.94 - ETA: 14:08 - loss: 0.1625 - acc: 0.94 - ETA: 14:06 - loss: 0.1622 - acc: 0.94 - ETA: 14:05 - loss: 0.1622 - acc: 0.94 - ETA: 14:03 - loss: 0.1621 - acc: 0.94 - ETA: 14:02 - loss: 0.1621 - acc: 0.94 - ETA: 14:00 - loss: 0.1619 - acc: 0.94 - ETA: 13:58 - loss: 0.1618 - acc: 0.94 - ETA: 13:57 - loss: 0.1618 - acc: 0.94 - ETA: 13:55 - loss: 0.1615 - acc: 0.94 - ETA: 13:54 - loss: 0.1614 - acc: 0.94 - ETA: 13:52 - loss: 0.1613 - acc: 0.94 - ETA: 13:51 - loss: 0.1613 - acc: 0.94 - ETA: 13:49 - loss: 0.1611 - acc: 0.94 - ETA: 13:48 - loss: 0.1610 - acc: 0.94 - ETA: 13:46 - loss: 0.1609 - acc: 0.94 - ETA: 13:45 - loss: 0.1611 - acc: 0.94 - ETA: 13:43 - loss: 0.1611 - acc: 0.94 - ETA: 13:42 - loss: 0.1609 - acc: 0.94 - ETA: 13:40 - loss: 0.1607 - acc: 0.94 - ETA: 13:39 - loss: 0.1606 - acc: 0.94 - ETA: 13:37 - loss: 0.1604 - acc: 0.94 - ETA: 13:36 - loss: 0.1602 - acc: 0.94 - ETA: 13:34 - loss: 0.1600 - acc: 0.94 - ETA: 13:33 - loss: 0.1599 - acc: 0.94 - ETA: 13:31 - loss: 0.1598 - acc: 0.94 - ETA: 13:30 - loss: 0.1597 - acc: 0.94 - ETA: 13:28 - loss: 0.1597 - acc: 0.94 - ETA: 13:27 - loss: 0.1595 - acc: 0.94 - ETA: 13:25 - loss: 0.1595 - acc: 0.94 - ETA: 13:23 - loss: 0.1593 - acc: 0.94 - ETA: 13:22 - loss: 0.1591 - acc: 0.94 - ETA: 13:20 - loss: 0.1590 - acc: 0.94 - ETA: 13:19 - loss: 0.1588 - acc: 0.94 - ETA: 13:17 - loss: 0.1587 - acc: 0.94 - ETA: 13:16 - loss: 0.1586 - acc: 0.94 - ETA: 13:14 - loss: 0.1583 - acc: 0.94 - ETA: 13:13 - loss: 0.1582 - acc: 0.94 - ETA: 13:11 - loss: 0.1581 - acc: 0.94 - ETA: 13:09 - loss: 0.1580 - acc: 0.94 - ETA: 13:08 - loss: 0.1577 - acc: 0.94 - ETA: 13:06 - loss: 0.1578 - acc: 0.94 - ETA: 13:05 - loss: 0.1576 - acc: 0.94 - ETA: 13:03 - loss: 0.1576 - acc: 0.94 - ETA: 13:01 - loss: 0.1575 - acc: 0.94 - ETA: 13:00 - loss: 0.1574 - acc: 0.94 - ETA: 12:58 - loss: 0.1572 - acc: 0.94 - ETA: 12:57 - loss: 0.1570 - acc: 0.94 - ETA: 12:55 - loss: 0.1568 - acc: 0.94 - ETA: 12:53 - loss: 0.1567 - acc: 0.94 - ETA: 12:52 - loss: 0.1567 - acc: 0.94 - ETA: 12:50 - loss: 0.1565 - acc: 0.94 - ETA: 12:49 - loss: 0.1564 - acc: 0.94 - ETA: 12:47 - loss: 0.1561 - acc: 0.94 - ETA: 12:46 - loss: 0.1560 - acc: 0.94 - ETA: 12:44 - loss: 0.1560 - acc: 0.94 - ETA: 12:42 - loss: 0.1560 - acc: 0.94 - ETA: 12:41 - loss: 0.1560 - acc: 0.94 - ETA: 12:39 - loss: 0.1560 - acc: 0.94 - ETA: 12:38 - loss: 0.1560 - acc: 0.94 - ETA: 12:36 - loss: 0.1558 - acc: 0.94 - ETA: 12:34 - loss: 0.1557 - acc: 0.94 - ETA: 12:33 - loss: 0.1555 - acc: 0.94 - ETA: 12:31 - loss: 0.1555 - acc: 0.94 - ETA: 12:29 - loss: 0.1554 - acc: 0.94 - ETA: 12:28 - loss: 0.1553 - acc: 0.94 - ETA: 12:26 - loss: 0.1552 - acc: 0.94 - ETA: 12:25 - loss: 0.1551 - acc: 0.94 - ETA: 12:23 - loss: 0.1550 - acc: 0.94 - ETA: 12:21 - loss: 0.1550 - acc: 0.94 - ETA: 12:20 - loss: 0.1550 - acc: 0.94 - ETA: 12:18 - loss: 0.1550 - acc: 0.94 - ETA: 12:17 - loss: 0.1548 - acc: 0.94 - ETA: 12:15 - loss: 0.1547 - acc: 0.94 - ETA: 12:14 - loss: 0.1546 - acc: 0.94 - ETA: 12:11 - loss: 0.1546 - acc: 0.94 - ETA: 12:10 - loss: 0.1546 - acc: 0.94 - ETA: 12:08 - loss: 0.1544 - acc: 0.94 - ETA: 12:06 - loss: 0.1545 - acc: 0.94 - ETA: 12:04 - loss: 0.1544 - acc: 0.94 - ETA: 12:02 - loss: 0.1544 - acc: 0.94 - ETA: 12:00 - loss: 0.1542 - acc: 0.94 - ETA: 11:59 - loss: 0.1543 - acc: 0.94 - ETA: 11:57 - loss: 0.1542 - acc: 0.94 - ETA: 11:55 - loss: 0.1542 - acc: 0.94 - ETA: 11:52 - loss: 0.1540 - acc: 0.94 - ETA: 11:50 - loss: 0.1539 - acc: 0.94 - ETA: 11:48 - loss: 0.1537 - acc: 0.94 - ETA: 11:47 - loss: 0.1536 - acc: 0.94 - ETA: 11:44 - loss: 0.1535 - acc: 0.94 - ETA: 11:42 - loss: 0.1534 - acc: 0.94 - ETA: 11:40 - loss: 0.1534 - acc: 0.94 - ETA: 11:38 - loss: 0.1532 - acc: 0.94 - ETA: 11:35 - loss: 0.1531 - acc: 0.94 - ETA: 11:33 - loss: 0.1529 - acc: 0.94 - ETA: 11:31 - loss: 0.1528 - acc: 0.94 - ETA: 11:29 - loss: 0.1527 - acc: 0.94 - ETA: 11:27 - loss: 0.1526 - acc: 0.94 - ETA: 11:25 - loss: 0.1525 - acc: 0.94 - ETA: 11:23 - loss: 0.1525 - acc: 0.94 - ETA: 11:21 - loss: 0.1523 - acc: 0.94 - ETA: 11:18 - loss: 0.1521 - acc: 0.94 - ETA: 11:16 - loss: 0.1520 - acc: 0.94 - ETA: 11:14 - loss: 0.1518 - acc: 0.94 - ETA: 11:12 - loss: 0.1517 - acc: 0.94 - ETA: 11:11 - loss: 0.1516 - acc: 0.94 - ETA: 11:09 - loss: 0.1516 - acc: 0.94 - ETA: 11:07 - loss: 0.1515 - acc: 0.94 - ETA: 11:06 - loss: 0.1514 - acc: 0.94 - ETA: 11:04 - loss: 0.1513 - acc: 0.94 - ETA: 11:02 - loss: 0.1512 - acc: 0.94 - ETA: 11:00 - loss: 0.1512 - acc: 0.94 - ETA: 10:58 - loss: 0.1510 - acc: 0.94 - ETA: 10:57 - loss: 0.1510 - acc: 0.94 - ETA: 10:55 - loss: 0.1508 - acc: 0.94 - ETA: 10:53 - loss: 0.1506 - acc: 0.94 - ETA: 10:51 - loss: 0.1505 - acc: 0.94 - ETA: 10:49 - loss: 0.1504 - acc: 0.94 - ETA: 10:47 - loss: 0.1504 - acc: 0.94 - ETA: 10:45 - loss: 0.1504 - acc: 0.94 - ETA: 10:43 - loss: 0.1503 - acc: 0.94 - ETA: 10:41 - loss: 0.1502 - acc: 0.94 - ETA: 10:39 - loss: 0.1501 - acc: 0.94 - ETA: 10:37 - loss: 0.1500 - acc: 0.94 - ETA: 10:35 - loss: 0.1500 - acc: 0.94 - ETA: 10:33 - loss: 0.1499 - acc: 0.94 - ETA: 10:31 - loss: 0.1499 - acc: 0.94 - ETA: 10:29 - loss: 0.1499 - acc: 0.94 - ETA: 10:27 - loss: 0.1499 - acc: 0.94 - ETA: 10:25 - loss: 0.1498 - acc: 0.94 - ETA: 10:23 - loss: 0.1495 - acc: 0.94 - ETA: 10:22 - loss: 0.1495 - acc: 0.94 - ETA: 10:20 - loss: 0.1495 - acc: 0.94 - ETA: 10:18 - loss: 0.1494 - acc: 0.94 - ETA: 10:16 - loss: 0.1493 - acc: 0.94 - ETA: 10:15 - loss: 0.1493 - acc: 0.94 - ETA: 10:13 - loss: 0.1493 - acc: 0.94 - ETA: 10:11 - loss: 0.1492 - acc: 0.94 - ETA: 10:09 - loss: 0.1493 - acc: 0.94 - ETA: 10:07 - loss: 0.1492 - acc: 0.94 - ETA: 10:05 - loss: 0.1492 - acc: 0.94 - ETA: 10:04 - loss: 0.1491 - acc: 0.94 - ETA: 10:02 - loss: 0.1490 - acc: 0.94 - ETA: 10:00 - loss: 0.1489 - acc: 0.94 - ETA: 9:58 - loss: 0.1489 - acc: 0.9452 - ETA: 9:56 - loss: 0.1488 - acc: 0.945 - ETA: 9:55 - loss: 0.1487 - acc: 0.945 - ETA: 9:53 - loss: 0.1488 - acc: 0.945 - ETA: 9:52 - loss: 0.1488 - acc: 0.945 - ETA: 9:50 - loss: 0.1488 - acc: 0.945 - ETA: 9:48 - loss: 0.1488 - acc: 0.945 - ETA: 9:47 - loss: 0.1487 - acc: 0.945 - ETA: 9:45 - loss: 0.1486 - acc: 0.945 - ETA: 9:44 - loss: 0.1486 - acc: 0.945 - ETA: 9:42 - loss: 0.1486 - acc: 0.945 - ETA: 9:40 - loss: 0.1485 - acc: 0.945 - ETA: 9:39 - loss: 0.1484 - acc: 0.945 - ETA: 9:37 - loss: 0.1483 - acc: 0.945 - ETA: 9:35 - loss: 0.1482 - acc: 0.945 - ETA: 9:34 - loss: 0.1481 - acc: 0.945 - ETA: 9:32 - loss: 0.1480 - acc: 0.945 - ETA: 9:30 - loss: 0.1479 - acc: 0.945 - ETA: 9:29 - loss: 0.1479 - acc: 0.945 - ETA: 9:27 - loss: 0.1476 - acc: 0.945 - ETA: 9:26 - loss: 0.1476 - acc: 0.945 - ETA: 9:24 - loss: 0.1475 - acc: 0.945 - ETA: 9:22 - loss: 0.1474 - acc: 0.945 - ETA: 9:20 - loss: 0.1472 - acc: 0.945 - ETA: 9:18 - loss: 0.1472 - acc: 0.945 - ETA: 9:16 - loss: 0.1473 - acc: 0.945 - ETA: 9:14 - loss: 0.1472 - acc: 0.945 - ETA: 9:12 - loss: 0.1470 - acc: 0.945 - ETA: 9:10 - loss: 0.1469 - acc: 0.945 - ETA: 9:08 - loss: 0.1469 - acc: 0.946 - ETA: 9:06 - loss: 0.1469 - acc: 0.945 - ETA: 9:05 - loss: 0.1469 - acc: 0.945 - ETA: 9:03 - loss: 0.1469 - acc: 0.946 - ETA: 9:01 - loss: 0.1469 - acc: 0.946 - ETA: 8:59 - loss: 0.1468 - acc: 0.946 - ETA: 8:57 - loss: 0.1469 - acc: 0.946 - ETA: 8:55 - loss: 0.1468 - acc: 0.946 - ETA: 8:53 - loss: 0.1468 - acc: 0.946 - ETA: 8:52 - loss: 0.1467 - acc: 0.946 - ETA: 8:50 - loss: 0.1466 - acc: 0.946 - ETA: 8:48 - loss: 0.1465 - acc: 0.946 - ETA: 8:46 - loss: 0.1464 - acc: 0.946 - ETA: 8:44 - loss: 0.1464 - acc: 0.946 - ETA: 8:42 - loss: 0.1464 - acc: 0.946 - ETA: 8:40 - loss: 0.1463 - acc: 0.946 - ETA: 8:39 - loss: 0.1463 - acc: 0.946 - ETA: 8:37 - loss: 0.1462 - acc: 0.946 - ETA: 8:35 - loss: 0.1462 - acc: 0.946 - ETA: 8:33 - loss: 0.1462 - acc: 0.946 - ETA: 8:31 - loss: 0.1461 - acc: 0.946 - ETA: 8:29 - loss: 0.1460 - acc: 0.946 - ETA: 8:28 - loss: 0.1459 - acc: 0.946 - ETA: 8:26 - loss: 0.1459 - acc: 0.946 - ETA: 8:24 - loss: 0.1457 - acc: 0.946 - ETA: 8:22 - loss: 0.1457 - acc: 0.946 - ETA: 8:21 - loss: 0.1456 - acc: 0.946 - ETA: 8:19 - loss: 0.1455 - acc: 0.946 - ETA: 8:17 - loss: 0.1456 - acc: 0.9463311296/360490 [========================>.....] - ETA: 8:15 - loss: 0.1454 - acc: 0.946 - ETA: 8:13 - loss: 0.1453 - acc: 0.946 - ETA: 8:11 - loss: 0.1452 - acc: 0.946 - ETA: 8:09 - loss: 0.1451 - acc: 0.946 - ETA: 8:07 - loss: 0.1452 - acc: 0.946 - ETA: 8:05 - loss: 0.1451 - acc: 0.946 - ETA: 8:04 - loss: 0.1451 - acc: 0.946 - ETA: 8:02 - loss: 0.1450 - acc: 0.946 - ETA: 8:00 - loss: 0.1450 - acc: 0.946 - ETA: 7:58 - loss: 0.1450 - acc: 0.946 - ETA: 7:56 - loss: 0.1450 - acc: 0.946 - ETA: 7:55 - loss: 0.1450 - acc: 0.946 - ETA: 7:53 - loss: 0.1449 - acc: 0.946 - ETA: 7:51 - loss: 0.1448 - acc: 0.946 - ETA: 7:49 - loss: 0.1448 - acc: 0.946 - ETA: 7:47 - loss: 0.1448 - acc: 0.946 - ETA: 7:45 - loss: 0.1447 - acc: 0.946 - ETA: 7:44 - loss: 0.1446 - acc: 0.946 - ETA: 7:42 - loss: 0.1447 - acc: 0.946 - ETA: 7:40 - loss: 0.1447 - acc: 0.946 - ETA: 7:39 - loss: 0.1447 - acc: 0.946 - ETA: 7:37 - loss: 0.1446 - acc: 0.946 - ETA: 7:35 - loss: 0.1446 - acc: 0.946 - ETA: 7:33 - loss: 0.1445 - acc: 0.946 - ETA: 7:31 - loss: 0.1445 - acc: 0.946 - ETA: 7:30 - loss: 0.1445 - acc: 0.946 - ETA: 7:28 - loss: 0.1444 - acc: 0.946 - ETA: 7:26 - loss: 0.1443 - acc: 0.946 - ETA: 7:24 - loss: 0.1443 - acc: 0.946 - ETA: 7:23 - loss: 0.1443 - acc: 0.946 - ETA: 7:21 - loss: 0.1443 - acc: 0.946 - ETA: 7:20 - loss: 0.1442 - acc: 0.946 - ETA: 7:18 - loss: 0.1441 - acc: 0.946 - ETA: 7:16 - loss: 0.1441 - acc: 0.946 - ETA: 7:15 - loss: 0.1441 - acc: 0.946 - ETA: 7:13 - loss: 0.1440 - acc: 0.946 - ETA: 7:12 - loss: 0.1439 - acc: 0.946 - ETA: 7:10 - loss: 0.1438 - acc: 0.946 - ETA: 7:08 - loss: 0.1437 - acc: 0.946 - ETA: 7:07 - loss: 0.1437 - acc: 0.946 - ETA: 7:05 - loss: 0.1437 - acc: 0.946 - ETA: 7:04 - loss: 0.1436 - acc: 0.946 - ETA: 7:02 - loss: 0.1435 - acc: 0.946 - ETA: 7:00 - loss: 0.1434 - acc: 0.946 - ETA: 6:59 - loss: 0.1434 - acc: 0.946 - ETA: 6:57 - loss: 0.1433 - acc: 0.946 - ETA: 6:56 - loss: 0.1433 - acc: 0.946 - ETA: 6:54 - loss: 0.1433 - acc: 0.946 - ETA: 6:53 - loss: 0.1433 - acc: 0.947 - ETA: 6:51 - loss: 0.1432 - acc: 0.947 - ETA: 6:49 - loss: 0.1432 - acc: 0.947 - ETA: 6:48 - loss: 0.1432 - acc: 0.947 - ETA: 6:46 - loss: 0.1432 - acc: 0.947 - ETA: 6:45 - loss: 0.1431 - acc: 0.947 - ETA: 6:43 - loss: 0.1431 - acc: 0.947 - ETA: 6:42 - loss: 0.1430 - acc: 0.947 - ETA: 6:40 - loss: 0.1431 - acc: 0.947 - ETA: 6:38 - loss: 0.1430 - acc: 0.947 - ETA: 6:37 - loss: 0.1429 - acc: 0.947 - ETA: 6:35 - loss: 0.1428 - acc: 0.947 - ETA: 6:34 - loss: 0.1427 - acc: 0.947 - ETA: 6:32 - loss: 0.1428 - acc: 0.947 - ETA: 6:30 - loss: 0.1428 - acc: 0.947 - ETA: 6:29 - loss: 0.1427 - acc: 0.947 - ETA: 6:27 - loss: 0.1426 - acc: 0.947 - ETA: 6:26 - loss: 0.1426 - acc: 0.947 - ETA: 6:24 - loss: 0.1426 - acc: 0.947 - ETA: 6:22 - loss: 0.1426 - acc: 0.947 - ETA: 6:21 - loss: 0.1425 - acc: 0.947 - ETA: 6:19 - loss: 0.1425 - acc: 0.947 - ETA: 6:18 - loss: 0.1424 - acc: 0.947 - ETA: 6:16 - loss: 0.1424 - acc: 0.947 - ETA: 6:14 - loss: 0.1423 - acc: 0.947 - ETA: 6:13 - loss: 0.1423 - acc: 0.947 - ETA: 6:11 - loss: 0.1423 - acc: 0.947 - ETA: 6:10 - loss: 0.1423 - acc: 0.947 - ETA: 6:08 - loss: 0.1423 - acc: 0.947 - ETA: 6:06 - loss: 0.1423 - acc: 0.947 - ETA: 6:05 - loss: 0.1423 - acc: 0.947 - ETA: 6:03 - loss: 0.1422 - acc: 0.947 - ETA: 6:01 - loss: 0.1422 - acc: 0.947 - ETA: 6:00 - loss: 0.1421 - acc: 0.947 - ETA: 5:58 - loss: 0.1421 - acc: 0.947 - ETA: 5:57 - loss: 0.1421 - acc: 0.947 - ETA: 5:55 - loss: 0.1420 - acc: 0.947 - ETA: 5:53 - loss: 0.1420 - acc: 0.947 - ETA: 5:52 - loss: 0.1419 - acc: 0.947 - ETA: 5:50 - loss: 0.1418 - acc: 0.947 - ETA: 5:48 - loss: 0.1418 - acc: 0.947 - ETA: 5:47 - loss: 0.1419 - acc: 0.947 - ETA: 5:45 - loss: 0.1418 - acc: 0.947 - ETA: 5:43 - loss: 0.1418 - acc: 0.947 - ETA: 5:41 - loss: 0.1417 - acc: 0.947 - ETA: 5:40 - loss: 0.1416 - acc: 0.947 - ETA: 5:38 - loss: 0.1417 - acc: 0.947 - ETA: 5:36 - loss: 0.1417 - acc: 0.947 - ETA: 5:35 - loss: 0.1416 - acc: 0.947 - ETA: 5:33 - loss: 0.1416 - acc: 0.947 - ETA: 5:32 - loss: 0.1416 - acc: 0.947 - ETA: 5:30 - loss: 0.1415 - acc: 0.947 - ETA: 5:28 - loss: 0.1415 - acc: 0.947 - ETA: 5:27 - loss: 0.1414 - acc: 0.947 - ETA: 5:25 - loss: 0.1413 - acc: 0.947 - ETA: 5:23 - loss: 0.1412 - acc: 0.947 - ETA: 5:21 - loss: 0.1412 - acc: 0.947 - ETA: 5:19 - loss: 0.1412 - acc: 0.947 - ETA: 5:17 - loss: 0.1412 - acc: 0.947 - ETA: 5:15 - loss: 0.1411 - acc: 0.947 - ETA: 1:10:50 - loss: 0.1411 - acc: 0.94 - ETA: 1:10:21 - loss: 0.1410 - acc: 0.94 - ETA: 1:09:52 - loss: 0.1410 - acc: 0.94 - ETA: 1:09:22 - loss: 0.1410 - acc: 0.94 - ETA: 1:08:53 - loss: 0.1410 - acc: 0.94 - ETA: 1:08:23 - loss: 0.1409 - acc: 0.94 - ETA: 1:07:54 - loss: 0.1408 - acc: 0.94 - ETA: 1:27:15 - loss: 0.1408 - acc: 0.94 - ETA: 1:26:36 - loss: 0.1408 - acc: 0.94 - ETA: 1:25:58 - loss: 0.1407 - acc: 0.94 - ETA: 1:25:20 - loss: 0.1406 - acc: 0.94 - ETA: 1:24:42 - loss: 0.1406 - acc: 0.94 - ETA: 1:24:04 - loss: 0.1406 - acc: 0.94 - ETA: 1:23:27 - loss: 0.1405 - acc: 0.94 - ETA: 1:22:49 - loss: 0.1405 - acc: 0.94 - ETA: 1:22:12 - loss: 0.1405 - acc: 0.94 - ETA: 1:21:34 - loss: 0.1405 - acc: 0.94 - ETA: 1:20:57 - loss: 0.1405 - acc: 0.94 - ETA: 1:20:20 - loss: 0.1405 - acc: 0.94 - ETA: 1:19:43 - loss: 0.1404 - acc: 0.94 - ETA: 1:19:06 - loss: 0.1404 - acc: 0.94 - ETA: 1:18:30 - loss: 0.1404 - acc: 0.94 - ETA: 1:17:53 - loss: 0.1403 - acc: 0.94 - ETA: 1:17:17 - loss: 0.1403 - acc: 0.94 - ETA: 1:16:41 - loss: 0.1403 - acc: 0.94 - ETA: 1:16:04 - loss: 0.1402 - acc: 0.94 - ETA: 1:15:28 - loss: 0.1402 - acc: 0.94 - ETA: 1:14:53 - loss: 0.1402 - acc: 0.94 - ETA: 1:14:17 - loss: 0.1403 - acc: 0.94 - ETA: 1:13:41 - loss: 0.1403 - acc: 0.94 - ETA: 1:13:06 - loss: 0.1402 - acc: 0.94 - ETA: 1:12:30 - loss: 0.1402 - acc: 0.94 - ETA: 1:11:55 - loss: 0.1402 - acc: 0.94 - ETA: 1:11:20 - loss: 0.1401 - acc: 0.94 - ETA: 1:10:45 - loss: 0.1401 - acc: 0.94 - ETA: 1:10:10 - loss: 0.1401 - acc: 0.94 - ETA: 1:09:35 - loss: 0.1400 - acc: 0.94 - ETA: 1:09:00 - loss: 0.1400 - acc: 0.94 - ETA: 1:08:26 - loss: 0.1399 - acc: 0.94 - ETA: 1:07:52 - loss: 0.1399 - acc: 0.94 - ETA: 1:07:17 - loss: 0.1399 - acc: 0.94 - ETA: 1:06:43 - loss: 0.1399 - acc: 0.94 - ETA: 1:06:09 - loss: 0.1398 - acc: 0.94 - ETA: 1:05:35 - loss: 0.1398 - acc: 0.94 - ETA: 1:05:01 - loss: 0.1398 - acc: 0.94 - ETA: 1:04:28 - loss: 0.1398 - acc: 0.94 - ETA: 1:03:54 - loss: 0.1397 - acc: 0.94 - ETA: 1:03:21 - loss: 0.1397 - acc: 0.94 - ETA: 1:02:47 - loss: 0.1396 - acc: 0.94 - ETA: 1:02:14 - loss: 0.1396 - acc: 0.94 - ETA: 1:01:41 - loss: 0.1395 - acc: 0.94 - ETA: 1:01:08 - loss: 0.1396 - acc: 0.94 - ETA: 1:00:35 - loss: 0.1396 - acc: 0.94 - ETA: 1:00:02 - loss: 0.1395 - acc: 0.94 - ETA: 59:29 - loss: 0.1395 - acc: 0.9480 - ETA: 58:57 - loss: 0.1394 - acc: 0.94 - ETA: 58:24 - loss: 0.1393 - acc: 0.94 - ETA: 57:52 - loss: 0.1394 - acc: 0.94 - ETA: 57:20 - loss: 0.1393 - acc: 0.94 - ETA: 56:47 - loss: 0.1394 - acc: 0.94 - ETA: 56:15 - loss: 0.1393 - acc: 0.94 - ETA: 55:43 - loss: 0.1393 - acc: 0.94 - ETA: 55:12 - loss: 0.1393 - acc: 0.94 - ETA: 54:40 - loss: 0.1392 - acc: 0.94 - ETA: 54:08 - loss: 0.1392 - acc: 0.94 - ETA: 53:37 - loss: 0.1392 - acc: 0.94 - ETA: 53:05 - loss: 0.1392 - acc: 0.94 - ETA: 52:34 - loss: 0.1391 - acc: 0.94 - ETA: 52:03 - loss: 0.1391 - acc: 0.94 - ETA: 51:32 - loss: 0.1390 - acc: 0.94 - ETA: 51:01 - loss: 0.1390 - acc: 0.94 - ETA: 50:30 - loss: 0.1390 - acc: 0.94 - ETA: 49:59 - loss: 0.1390 - acc: 0.94 - ETA: 49:29 - loss: 0.1389 - acc: 0.94 - ETA: 48:58 - loss: 0.1389 - acc: 0.94 - ETA: 48:27 - loss: 0.1390 - acc: 0.94 - ETA: 47:57 - loss: 0.1389 - acc: 0.94 - ETA: 47:27 - loss: 0.1389 - acc: 0.94 - ETA: 46:57 - loss: 0.1389 - acc: 0.94 - ETA: 46:27 - loss: 0.1388 - acc: 0.94 - ETA: 45:57 - loss: 0.1388 - acc: 0.94 - ETA: 45:27 - loss: 0.1387 - acc: 0.94 - ETA: 44:57 - loss: 0.1387 - acc: 0.94 - ETA: 44:27 - loss: 0.1387 - acc: 0.94 - ETA: 43:58 - loss: 0.1387 - acc: 0.94 - ETA: 43:28 - loss: 0.1386 - acc: 0.94 - ETA: 42:59 - loss: 0.1385 - acc: 0.94 - ETA: 42:29 - loss: 0.1385 - acc: 0.94 - ETA: 42:00 - loss: 0.1385 - acc: 0.94 - ETA: 41:31 - loss: 0.1384 - acc: 0.94 - ETA: 41:02 - loss: 0.1384 - acc: 0.94 - ETA: 40:33 - loss: 0.1384 - acc: 0.94 - ETA: 40:04 - loss: 0.1384 - acc: 0.9483360490/360490 [==============================] - ETA: 39:35 - loss: 0.1384 - acc: 0.94 - ETA: 39:07 - loss: 0.1383 - acc: 0.94 - ETA: 38:38 - loss: 0.1383 - acc: 0.94 - ETA: 38:10 - loss: 0.1382 - acc: 0.94 - ETA: 37:41 - loss: 0.1382 - acc: 0.94 - ETA: 37:13 - loss: 0.1382 - acc: 0.94 - ETA: 36:45 - loss: 0.1382 - acc: 0.94 - ETA: 36:16 - loss: 0.1381 - acc: 0.94 - ETA: 35:48 - loss: 0.1381 - acc: 0.94 - ETA: 35:20 - loss: 0.1381 - acc: 0.94 - ETA: 34:52 - loss: 0.1381 - acc: 0.94 - ETA: 34:25 - loss: 0.1380 - acc: 0.94 - ETA: 33:57 - loss: 0.1380 - acc: 0.94 - ETA: 33:29 - loss: 0.1379 - acc: 0.94 - ETA: 33:02 - loss: 0.1379 - acc: 0.94 - ETA: 32:34 - loss: 0.1379 - acc: 0.94 - ETA: 32:07 - loss: 0.1379 - acc: 0.94 - ETA: 31:40 - loss: 0.1379 - acc: 0.94 - ETA: 31:13 - loss: 0.1378 - acc: 0.94 - ETA: 30:45 - loss: 0.1378 - acc: 0.94 - ETA: 30:18 - loss: 0.1378 - acc: 0.94 - ETA: 29:51 - loss: 0.1377 - acc: 0.94 - ETA: 29:25 - loss: 0.1377 - acc: 0.94 - ETA: 28:58 - loss: 0.1377 - acc: 0.94 - ETA: 28:31 - loss: 0.1376 - acc: 0.94 - ETA: 28:04 - loss: 0.1376 - acc: 0.94 - ETA: 27:38 - loss: 0.1376 - acc: 0.94 - ETA: 27:11 - loss: 0.1376 - acc: 0.94 - ETA: 26:45 - loss: 0.1375 - acc: 0.94 - ETA: 26:19 - loss: 0.1375 - acc: 0.94 - ETA: 25:53 - loss: 0.1375 - acc: 0.94 - ETA: 25:26 - loss: 0.1375 - acc: 0.94 - ETA: 25:00 - loss: 0.1375 - acc: 0.94 - ETA: 24:34 - loss: 0.1374 - acc: 0.94 - ETA: 24:08 - loss: 0.1374 - acc: 0.94 - ETA: 23:43 - loss: 0.1374 - acc: 0.94 - ETA: 23:17 - loss: 0.1375 - acc: 0.94 - ETA: 22:51 - loss: 0.1374 - acc: 0.94 - ETA: 22:26 - loss: 0.1374 - acc: 0.94 - ETA: 22:00 - loss: 0.1373 - acc: 0.94 - ETA: 21:35 - loss: 0.1373 - acc: 0.94 - ETA: 21:09 - loss: 0.1373 - acc: 0.94 - ETA: 20:44 - loss: 0.1372 - acc: 0.94 - ETA: 20:19 - loss: 0.1372 - acc: 0.94 - ETA: 19:53 - loss: 0.1372 - acc: 0.94 - ETA: 19:28 - loss: 0.1372 - acc: 0.94 - ETA: 19:03 - loss: 0.1372 - acc: 0.94 - ETA: 18:38 - loss: 0.1372 - acc: 0.94 - ETA: 18:14 - loss: 0.1372 - acc: 0.94 - ETA: 17:49 - loss: 0.1372 - acc: 0.94 - ETA: 17:24 - loss: 0.1371 - acc: 0.94 - ETA: 16:59 - loss: 0.1370 - acc: 0.94 - ETA: 16:35 - loss: 0.1370 - acc: 0.94 - ETA: 16:10 - loss: 0.1370 - acc: 0.94 - ETA: 15:46 - loss: 0.1370 - acc: 0.94 - ETA: 15:22 - loss: 0.1369 - acc: 0.94 - ETA: 14:57 - loss: 0.1369 - acc: 0.94 - ETA: 14:33 - loss: 0.1369 - acc: 0.94 - ETA: 14:09 - loss: 0.1369 - acc: 0.94 - ETA: 13:45 - loss: 0.1368 - acc: 0.94 - ETA: 13:21 - loss: 0.1368 - acc: 0.94 - ETA: 12:57 - loss: 0.1368 - acc: 0.94 - ETA: 12:33 - loss: 0.1367 - acc: 0.94 - ETA: 12:09 - loss: 0.1367 - acc: 0.94 - ETA: 11:46 - loss: 0.1367 - acc: 0.94 - ETA: 11:22 - loss: 0.1366 - acc: 0.94 - ETA: 10:58 - loss: 0.1366 - acc: 0.94 - ETA: 10:35 - loss: 0.1366 - acc: 0.94 - ETA: 10:11 - loss: 0.1366 - acc: 0.94 - ETA: 9:48 - loss: 0.1365 - acc: 0.9487 - ETA: 9:24 - loss: 0.1365 - acc: 0.948 - ETA: 9:01 - loss: 0.1365 - acc: 0.948 - ETA: 8:38 - loss: 0.1365 - acc: 0.948 - ETA: 8:15 - loss: 0.1365 - acc: 0.948 - ETA: 7:52 - loss: 0.1365 - acc: 0.948 - ETA: 7:29 - loss: 0.1364 - acc: 0.948 - ETA: 7:06 - loss: 0.1364 - acc: 0.948 - ETA: 6:43 - loss: 0.1364 - acc: 0.948 - ETA: 6:20 - loss: 0.1364 - acc: 0.948 - ETA: 5:57 - loss: 0.1364 - acc: 0.948 - ETA: 5:35 - loss: 0.1364 - acc: 0.948 - ETA: 5:12 - loss: 0.1363 - acc: 0.948 - ETA: 4:49 - loss: 0.1363 - acc: 0.948 - ETA: 4:27 - loss: 0.1363 - acc: 0.948 - ETA: 4:04 - loss: 0.1363 - acc: 0.948 - ETA: 3:42 - loss: 0.1363 - acc: 0.948 - ETA: 3:20 - loss: 0.1362 - acc: 0.948 - ETA: 2:57 - loss: 0.1362 - acc: 0.948 - ETA: 2:35 - loss: 0.1362 - acc: 0.948 - ETA: 2:13 - loss: 0.1362 - acc: 0.948 - ETA: 1:51 - loss: 0.1361 - acc: 0.948 - ETA: 1:29 - loss: 0.1361 - acc: 0.948 - ETA: 1:07 - loss: 0.1361 - acc: 0.948 - ETA: 45s - loss: 0.1361 - acc: 0.948 - ETA: 23s - loss: 0.1360 - acc: 0.94 - ETA: 1s - loss: 0.1360 - acc: 0.9487 - 15340s 43ms/step - loss: 0.1360 - acc: 0.9487 - val_loss: 0.1205 - val_acc: 0.9525\n",
      "Val F1 Score: 0.6191\n",
      "Train on 360490 samples, validate on 31347 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104448/360490 [=======>......................] - ETA: 12:50 - loss: 0.0975 - acc: 0.97 - ETA: 12:27 - loss: 0.1019 - acc: 0.96 - ETA: 12:43 - loss: 0.0891 - acc: 0.96 - ETA: 12:43 - loss: 0.0901 - acc: 0.96 - ETA: 12:52 - loss: 0.0945 - acc: 0.96 - ETA: 13:02 - loss: 0.0883 - acc: 0.96 - ETA: 12:58 - loss: 0.0901 - acc: 0.96 - ETA: 12:55 - loss: 0.0896 - acc: 0.96 - ETA: 12:54 - loss: 0.0899 - acc: 0.96 - ETA: 12:50 - loss: 0.0893 - acc: 0.96 - ETA: 12:46 - loss: 0.0921 - acc: 0.96 - ETA: 12:45 - loss: 0.0925 - acc: 0.96 - ETA: 12:41 - loss: 0.0943 - acc: 0.96 - ETA: 12:41 - loss: 0.0951 - acc: 0.96 - ETA: 12:37 - loss: 0.0978 - acc: 0.96 - ETA: 12:34 - loss: 0.0988 - acc: 0.96 - ETA: 12:34 - loss: 0.0994 - acc: 0.96 - ETA: 12:32 - loss: 0.0985 - acc: 0.96 - ETA: 12:33 - loss: 0.0983 - acc: 0.96 - ETA: 12:31 - loss: 0.0982 - acc: 0.96 - ETA: 12:29 - loss: 0.0976 - acc: 0.96 - ETA: 12:27 - loss: 0.0981 - acc: 0.96 - ETA: 12:25 - loss: 0.0986 - acc: 0.96 - ETA: 12:24 - loss: 0.0991 - acc: 0.96 - ETA: 12:22 - loss: 0.0974 - acc: 0.96 - ETA: 12:20 - loss: 0.0977 - acc: 0.96 - ETA: 12:20 - loss: 0.0977 - acc: 0.96 - ETA: 12:17 - loss: 0.0973 - acc: 0.96 - ETA: 12:16 - loss: 0.0977 - acc: 0.96 - ETA: 12:15 - loss: 0.0977 - acc: 0.96 - ETA: 12:12 - loss: 0.0975 - acc: 0.96 - ETA: 12:11 - loss: 0.0982 - acc: 0.96 - ETA: 12:11 - loss: 0.0982 - acc: 0.96 - ETA: 12:10 - loss: 0.0980 - acc: 0.96 - ETA: 12:09 - loss: 0.0982 - acc: 0.96 - ETA: 12:09 - loss: 0.0981 - acc: 0.96 - ETA: 12:08 - loss: 0.0979 - acc: 0.96 - ETA: 12:07 - loss: 0.0985 - acc: 0.96 - ETA: 12:06 - loss: 0.0985 - acc: 0.96 - ETA: 12:04 - loss: 0.0982 - acc: 0.96 - ETA: 12:03 - loss: 0.0981 - acc: 0.96 - ETA: 12:01 - loss: 0.0977 - acc: 0.96 - ETA: 11:59 - loss: 0.0978 - acc: 0.96 - ETA: 11:59 - loss: 0.0977 - acc: 0.96 - ETA: 11:59 - loss: 0.0978 - acc: 0.96 - ETA: 11:58 - loss: 0.0984 - acc: 0.96 - ETA: 11:56 - loss: 0.0984 - acc: 0.96 - ETA: 11:55 - loss: 0.0988 - acc: 0.96 - ETA: 11:54 - loss: 0.0989 - acc: 0.96 - ETA: 11:52 - loss: 0.0989 - acc: 0.96 - ETA: 11:50 - loss: 0.0989 - acc: 0.96 - ETA: 11:48 - loss: 0.0985 - acc: 0.96 - ETA: 11:46 - loss: 0.0982 - acc: 0.96 - ETA: 11:45 - loss: 0.0983 - acc: 0.96 - ETA: 11:43 - loss: 0.0983 - acc: 0.96 - ETA: 11:42 - loss: 0.0982 - acc: 0.96 - ETA: 11:41 - loss: 0.0983 - acc: 0.96 - ETA: 11:39 - loss: 0.0984 - acc: 0.96 - ETA: 11:38 - loss: 0.0983 - acc: 0.96 - ETA: 11:37 - loss: 0.0981 - acc: 0.96 - ETA: 11:35 - loss: 0.0978 - acc: 0.96 - ETA: 11:34 - loss: 0.0979 - acc: 0.96 - ETA: 11:32 - loss: 0.0991 - acc: 0.96 - ETA: 11:31 - loss: 0.0990 - acc: 0.96 - ETA: 11:30 - loss: 0.0995 - acc: 0.96 - ETA: 11:28 - loss: 0.0997 - acc: 0.96 - ETA: 11:28 - loss: 0.0997 - acc: 0.96 - ETA: 11:27 - loss: 0.1001 - acc: 0.96 - ETA: 11:25 - loss: 0.1004 - acc: 0.96 - ETA: 11:24 - loss: 0.1006 - acc: 0.96 - ETA: 11:22 - loss: 0.1007 - acc: 0.96 - ETA: 11:21 - loss: 0.1007 - acc: 0.96 - ETA: 11:20 - loss: 0.1007 - acc: 0.96 - ETA: 11:19 - loss: 0.1009 - acc: 0.96 - ETA: 11:17 - loss: 0.1011 - acc: 0.96 - ETA: 11:16 - loss: 0.1012 - acc: 0.96 - ETA: 11:14 - loss: 0.1010 - acc: 0.96 - ETA: 11:13 - loss: 0.1013 - acc: 0.96 - ETA: 11:12 - loss: 0.1011 - acc: 0.96 - ETA: 11:10 - loss: 0.1015 - acc: 0.96 - ETA: 11:10 - loss: 0.1013 - acc: 0.96 - ETA: 11:08 - loss: 0.1013 - acc: 0.96 - ETA: 11:07 - loss: 0.1011 - acc: 0.96 - ETA: 11:05 - loss: 0.1012 - acc: 0.96 - ETA: 11:04 - loss: 0.1009 - acc: 0.96 - ETA: 11:03 - loss: 0.1010 - acc: 0.96 - ETA: 11:02 - loss: 0.1010 - acc: 0.96 - ETA: 11:00 - loss: 0.1006 - acc: 0.96 - ETA: 10:59 - loss: 0.1007 - acc: 0.96 - ETA: 10:58 - loss: 0.1008 - acc: 0.96 - ETA: 10:57 - loss: 0.1003 - acc: 0.96 - ETA: 10:55 - loss: 0.1004 - acc: 0.96 - ETA: 10:54 - loss: 0.1006 - acc: 0.96 - ETA: 10:53 - loss: 0.1003 - acc: 0.96 - ETA: 10:52 - loss: 0.1003 - acc: 0.96 - ETA: 10:51 - loss: 0.1004 - acc: 0.96 - ETA: 10:50 - loss: 0.1005 - acc: 0.96 - ETA: 10:49 - loss: 0.1006 - acc: 0.96 - ETA: 10:48 - loss: 0.1003 - acc: 0.96 - ETA: 10:46 - loss: 0.1005 - acc: 0.96 - ETA: 10:45 - loss: 0.1002 - acc: 0.96 - ETA: 10:44 - loss: 0.1003 - acc: 0.96 - ETA: 10:44 - loss: 0.1004 - acc: 0.96 - ETA: 10:43 - loss: 0.1006 - acc: 0.96 - ETA: 10:42 - loss: 0.1004 - acc: 0.96 - ETA: 10:41 - loss: 0.1000 - acc: 0.96 - ETA: 10:39 - loss: 0.0999 - acc: 0.96 - ETA: 10:39 - loss: 0.0999 - acc: 0.96 - ETA: 10:38 - loss: 0.1001 - acc: 0.96 - ETA: 10:37 - loss: 0.0998 - acc: 0.96 - ETA: 10:36 - loss: 0.0998 - acc: 0.96 - ETA: 10:35 - loss: 0.0997 - acc: 0.96 - ETA: 10:34 - loss: 0.0998 - acc: 0.96 - ETA: 10:33 - loss: 0.0998 - acc: 0.96 - ETA: 10:31 - loss: 0.0998 - acc: 0.96 - ETA: 10:30 - loss: 0.0999 - acc: 0.96 - ETA: 10:29 - loss: 0.0998 - acc: 0.96 - ETA: 10:28 - loss: 0.0999 - acc: 0.96 - ETA: 10:27 - loss: 0.0999 - acc: 0.96 - ETA: 10:26 - loss: 0.1000 - acc: 0.96 - ETA: 10:24 - loss: 0.1001 - acc: 0.96 - ETA: 10:23 - loss: 0.1000 - acc: 0.96 - ETA: 10:22 - loss: 0.0998 - acc: 0.96 - ETA: 10:21 - loss: 0.0997 - acc: 0.96 - ETA: 10:20 - loss: 0.0995 - acc: 0.96 - ETA: 10:19 - loss: 0.0996 - acc: 0.96 - ETA: 10:18 - loss: 0.0993 - acc: 0.96 - ETA: 10:18 - loss: 0.0995 - acc: 0.96 - ETA: 10:17 - loss: 0.0994 - acc: 0.96 - ETA: 10:16 - loss: 0.0997 - acc: 0.96 - ETA: 10:15 - loss: 0.0997 - acc: 0.96 - ETA: 10:14 - loss: 0.0997 - acc: 0.96 - ETA: 10:13 - loss: 0.0996 - acc: 0.96 - ETA: 10:12 - loss: 0.0995 - acc: 0.96 - ETA: 10:11 - loss: 0.0993 - acc: 0.96 - ETA: 10:10 - loss: 0.0994 - acc: 0.96 - ETA: 10:09 - loss: 0.0998 - acc: 0.96 - ETA: 10:09 - loss: 0.0997 - acc: 0.96 - ETA: 10:08 - loss: 0.0998 - acc: 0.96 - ETA: 10:07 - loss: 0.0997 - acc: 0.96 - ETA: 10:06 - loss: 0.0998 - acc: 0.96 - ETA: 10:05 - loss: 0.0998 - acc: 0.96 - ETA: 10:04 - loss: 0.0999 - acc: 0.96 - ETA: 10:04 - loss: 0.0999 - acc: 0.96 - ETA: 10:03 - loss: 0.0998 - acc: 0.96 - ETA: 10:02 - loss: 0.1000 - acc: 0.96 - ETA: 10:01 - loss: 0.1001 - acc: 0.96 - ETA: 10:00 - loss: 0.1001 - acc: 0.96 - ETA: 9:59 - loss: 0.1000 - acc: 0.9614 - ETA: 9:58 - loss: 0.0999 - acc: 0.961 - ETA: 9:57 - loss: 0.0999 - acc: 0.961 - ETA: 9:56 - loss: 0.1001 - acc: 0.961 - ETA: 9:55 - loss: 0.1001 - acc: 0.961 - ETA: 9:55 - loss: 0.0999 - acc: 0.961 - ETA: 9:54 - loss: 0.1000 - acc: 0.961 - ETA: 9:53 - loss: 0.1000 - acc: 0.961 - ETA: 9:52 - loss: 0.1001 - acc: 0.961 - ETA: 9:51 - loss: 0.1001 - acc: 0.961 - ETA: 9:50 - loss: 0.1000 - acc: 0.961 - ETA: 9:49 - loss: 0.0999 - acc: 0.961 - ETA: 9:49 - loss: 0.0999 - acc: 0.961 - ETA: 9:48 - loss: 0.0998 - acc: 0.961 - ETA: 9:47 - loss: 0.0997 - acc: 0.961 - ETA: 9:46 - loss: 0.0997 - acc: 0.961 - ETA: 9:45 - loss: 0.0996 - acc: 0.961 - ETA: 9:44 - loss: 0.0998 - acc: 0.961 - ETA: 9:43 - loss: 0.0997 - acc: 0.961 - ETA: 9:42 - loss: 0.0996 - acc: 0.961 - ETA: 9:41 - loss: 0.0996 - acc: 0.961 - ETA: 9:40 - loss: 0.0995 - acc: 0.961 - ETA: 9:39 - loss: 0.0996 - acc: 0.961 - ETA: 9:38 - loss: 0.0997 - acc: 0.961 - ETA: 9:37 - loss: 0.0998 - acc: 0.961 - ETA: 9:36 - loss: 0.1000 - acc: 0.961 - ETA: 9:35 - loss: 0.1000 - acc: 0.961 - ETA: 9:34 - loss: 0.0999 - acc: 0.961 - ETA: 9:33 - loss: 0.1000 - acc: 0.961 - ETA: 9:33 - loss: 0.0999 - acc: 0.961 - ETA: 9:32 - loss: 0.1001 - acc: 0.961 - ETA: 9:31 - loss: 0.1003 - acc: 0.961 - ETA: 9:30 - loss: 0.1005 - acc: 0.961 - ETA: 9:29 - loss: 0.1006 - acc: 0.961 - ETA: 9:28 - loss: 0.1005 - acc: 0.961 - ETA: 9:27 - loss: 0.1005 - acc: 0.961 - ETA: 9:26 - loss: 0.1005 - acc: 0.961 - ETA: 9:25 - loss: 0.1005 - acc: 0.961 - ETA: 9:24 - loss: 0.1005 - acc: 0.961 - ETA: 9:24 - loss: 0.1008 - acc: 0.961 - ETA: 9:23 - loss: 0.1007 - acc: 0.961 - ETA: 9:23 - loss: 0.1006 - acc: 0.961 - ETA: 9:22 - loss: 0.1005 - acc: 0.961 - ETA: 9:22 - loss: 0.1006 - acc: 0.961 - ETA: 9:21 - loss: 0.1005 - acc: 0.961 - ETA: 9:20 - loss: 0.1004 - acc: 0.961 - ETA: 9:20 - loss: 0.1005 - acc: 0.961 - ETA: 9:19 - loss: 0.1004 - acc: 0.961 - ETA: 9:18 - loss: 0.1003 - acc: 0.961 - ETA: 9:17 - loss: 0.1002 - acc: 0.961 - ETA: 9:16 - loss: 0.1003 - acc: 0.961 - ETA: 9:15 - loss: 0.1004 - acc: 0.961 - ETA: 9:14 - loss: 0.1003 - acc: 0.961 - ETA: 9:13 - loss: 0.1003 - acc: 0.961 - ETA: 9:13 - loss: 0.1003 - acc: 0.961 - ETA: 9:12 - loss: 0.1003 - acc: 0.9615208384/360490 [================>.............] - ETA: 9:11 - loss: 0.1004 - acc: 0.961 - ETA: 9:10 - loss: 0.1005 - acc: 0.961 - ETA: 9:09 - loss: 0.1005 - acc: 0.961 - ETA: 9:08 - loss: 0.1004 - acc: 0.961 - ETA: 9:07 - loss: 0.1003 - acc: 0.961 - ETA: 9:06 - loss: 0.1003 - acc: 0.961 - ETA: 9:05 - loss: 0.1002 - acc: 0.961 - ETA: 9:04 - loss: 0.1002 - acc: 0.961 - ETA: 9:03 - loss: 0.1003 - acc: 0.961 - ETA: 9:03 - loss: 0.1003 - acc: 0.961 - ETA: 9:02 - loss: 0.1004 - acc: 0.961 - ETA: 9:01 - loss: 0.1004 - acc: 0.961 - ETA: 9:00 - loss: 0.1004 - acc: 0.961 - ETA: 8:59 - loss: 0.1005 - acc: 0.961 - ETA: 8:59 - loss: 0.1006 - acc: 0.961 - ETA: 8:58 - loss: 0.1005 - acc: 0.961 - ETA: 8:57 - loss: 0.1005 - acc: 0.961 - ETA: 8:56 - loss: 0.1004 - acc: 0.961 - ETA: 8:55 - loss: 0.1004 - acc: 0.961 - ETA: 8:54 - loss: 0.1004 - acc: 0.961 - ETA: 8:53 - loss: 0.1005 - acc: 0.961 - ETA: 8:52 - loss: 0.1006 - acc: 0.961 - ETA: 8:52 - loss: 0.1005 - acc: 0.961 - ETA: 8:51 - loss: 0.1005 - acc: 0.961 - ETA: 8:50 - loss: 0.1005 - acc: 0.961 - ETA: 8:49 - loss: 0.1004 - acc: 0.961 - ETA: 8:48 - loss: 0.1003 - acc: 0.961 - ETA: 8:47 - loss: 0.1003 - acc: 0.961 - ETA: 8:46 - loss: 0.1004 - acc: 0.961 - ETA: 8:46 - loss: 0.1004 - acc: 0.961 - ETA: 8:45 - loss: 0.1005 - acc: 0.961 - ETA: 8:44 - loss: 0.1007 - acc: 0.961 - ETA: 8:43 - loss: 0.1006 - acc: 0.961 - ETA: 8:42 - loss: 0.1005 - acc: 0.961 - ETA: 8:41 - loss: 0.1003 - acc: 0.961 - ETA: 8:40 - loss: 0.1003 - acc: 0.961 - ETA: 8:39 - loss: 0.1003 - acc: 0.961 - ETA: 8:38 - loss: 0.1002 - acc: 0.961 - ETA: 8:37 - loss: 0.1000 - acc: 0.961 - ETA: 8:36 - loss: 0.0999 - acc: 0.961 - ETA: 8:35 - loss: 0.1000 - acc: 0.961 - ETA: 8:35 - loss: 0.1000 - acc: 0.961 - ETA: 8:34 - loss: 0.1000 - acc: 0.961 - ETA: 8:33 - loss: 0.0999 - acc: 0.961 - ETA: 8:32 - loss: 0.1000 - acc: 0.961 - ETA: 8:31 - loss: 0.1000 - acc: 0.961 - ETA: 8:30 - loss: 0.1000 - acc: 0.961 - ETA: 8:29 - loss: 0.1000 - acc: 0.961 - ETA: 8:28 - loss: 0.1000 - acc: 0.961 - ETA: 8:27 - loss: 0.1000 - acc: 0.961 - ETA: 8:26 - loss: 0.0999 - acc: 0.961 - ETA: 8:25 - loss: 0.0999 - acc: 0.961 - ETA: 8:24 - loss: 0.0999 - acc: 0.961 - ETA: 8:23 - loss: 0.0999 - acc: 0.961 - ETA: 8:22 - loss: 0.0999 - acc: 0.961 - ETA: 8:21 - loss: 0.0999 - acc: 0.961 - ETA: 8:20 - loss: 0.0999 - acc: 0.961 - ETA: 8:19 - loss: 0.0998 - acc: 0.961 - ETA: 8:18 - loss: 0.0997 - acc: 0.961 - ETA: 8:17 - loss: 0.0996 - acc: 0.961 - ETA: 8:17 - loss: 0.0997 - acc: 0.961 - ETA: 8:16 - loss: 0.0997 - acc: 0.961 - ETA: 8:15 - loss: 0.0997 - acc: 0.961 - ETA: 8:14 - loss: 0.0997 - acc: 0.961 - ETA: 8:13 - loss: 0.0997 - acc: 0.961 - ETA: 8:12 - loss: 0.0998 - acc: 0.961 - ETA: 8:11 - loss: 0.0998 - acc: 0.961 - ETA: 8:10 - loss: 0.0999 - acc: 0.961 - ETA: 8:09 - loss: 0.0998 - acc: 0.961 - ETA: 8:08 - loss: 0.0998 - acc: 0.961 - ETA: 8:07 - loss: 0.0998 - acc: 0.961 - ETA: 8:05 - loss: 0.0997 - acc: 0.961 - ETA: 8:04 - loss: 0.0996 - acc: 0.961 - ETA: 8:03 - loss: 0.0996 - acc: 0.961 - ETA: 8:02 - loss: 0.0996 - acc: 0.961 - ETA: 8:01 - loss: 0.0995 - acc: 0.961 - ETA: 8:00 - loss: 0.0995 - acc: 0.961 - ETA: 7:59 - loss: 0.0995 - acc: 0.961 - ETA: 7:58 - loss: 0.0994 - acc: 0.961 - ETA: 7:57 - loss: 0.0995 - acc: 0.961 - ETA: 7:56 - loss: 0.0994 - acc: 0.961 - ETA: 7:55 - loss: 0.0994 - acc: 0.961 - ETA: 7:54 - loss: 0.0994 - acc: 0.961 - ETA: 7:53 - loss: 0.0994 - acc: 0.961 - ETA: 7:52 - loss: 0.0994 - acc: 0.961 - ETA: 7:51 - loss: 0.0994 - acc: 0.961 - ETA: 7:50 - loss: 0.0993 - acc: 0.961 - ETA: 7:49 - loss: 0.0993 - acc: 0.961 - ETA: 7:48 - loss: 0.0992 - acc: 0.961 - ETA: 7:47 - loss: 0.0992 - acc: 0.961 - ETA: 7:46 - loss: 0.0991 - acc: 0.961 - ETA: 7:45 - loss: 0.0992 - acc: 0.961 - ETA: 7:44 - loss: 0.0993 - acc: 0.961 - ETA: 7:43 - loss: 0.0993 - acc: 0.961 - ETA: 7:42 - loss: 0.0992 - acc: 0.961 - ETA: 7:41 - loss: 0.0992 - acc: 0.961 - ETA: 7:40 - loss: 0.0992 - acc: 0.961 - ETA: 7:39 - loss: 0.0991 - acc: 0.961 - ETA: 7:38 - loss: 0.0990 - acc: 0.961 - ETA: 7:37 - loss: 0.0991 - acc: 0.961 - ETA: 7:36 - loss: 0.0991 - acc: 0.961 - ETA: 7:35 - loss: 0.0992 - acc: 0.961 - ETA: 7:34 - loss: 0.0993 - acc: 0.961 - ETA: 7:33 - loss: 0.0992 - acc: 0.961 - ETA: 7:32 - loss: 0.0992 - acc: 0.961 - ETA: 7:31 - loss: 0.0992 - acc: 0.961 - ETA: 7:30 - loss: 0.0993 - acc: 0.961 - ETA: 7:29 - loss: 0.0994 - acc: 0.961 - ETA: 7:27 - loss: 0.0993 - acc: 0.961 - ETA: 7:26 - loss: 0.0993 - acc: 0.961 - ETA: 7:25 - loss: 0.0993 - acc: 0.961 - ETA: 7:24 - loss: 0.0993 - acc: 0.961 - ETA: 7:23 - loss: 0.0994 - acc: 0.961 - ETA: 7:22 - loss: 0.0994 - acc: 0.961 - ETA: 7:21 - loss: 0.0994 - acc: 0.961 - ETA: 7:20 - loss: 0.0994 - acc: 0.961 - ETA: 7:19 - loss: 0.0995 - acc: 0.961 - ETA: 7:18 - loss: 0.0995 - acc: 0.961 - ETA: 7:17 - loss: 0.0995 - acc: 0.961 - ETA: 7:16 - loss: 0.0996 - acc: 0.961 - ETA: 7:15 - loss: 0.0995 - acc: 0.961 - ETA: 7:14 - loss: 0.0995 - acc: 0.961 - ETA: 7:13 - loss: 0.0994 - acc: 0.961 - ETA: 7:12 - loss: 0.0994 - acc: 0.961 - ETA: 7:11 - loss: 0.0994 - acc: 0.961 - ETA: 7:10 - loss: 0.0994 - acc: 0.961 - ETA: 7:09 - loss: 0.0995 - acc: 0.961 - ETA: 7:08 - loss: 0.0994 - acc: 0.961 - ETA: 7:07 - loss: 0.0994 - acc: 0.961 - ETA: 7:06 - loss: 0.0993 - acc: 0.961 - ETA: 7:05 - loss: 0.0992 - acc: 0.961 - ETA: 7:04 - loss: 0.0993 - acc: 0.961 - ETA: 7:03 - loss: 0.0993 - acc: 0.961 - ETA: 7:01 - loss: 0.0994 - acc: 0.961 - ETA: 7:00 - loss: 0.0995 - acc: 0.961 - ETA: 6:59 - loss: 0.0994 - acc: 0.961 - ETA: 6:58 - loss: 0.0995 - acc: 0.961 - ETA: 6:57 - loss: 0.0995 - acc: 0.961 - ETA: 6:56 - loss: 0.0995 - acc: 0.961 - ETA: 6:55 - loss: 0.0995 - acc: 0.961 - ETA: 6:54 - loss: 0.0995 - acc: 0.961 - ETA: 6:54 - loss: 0.0995 - acc: 0.961 - ETA: 6:53 - loss: 0.0995 - acc: 0.961 - ETA: 6:52 - loss: 0.0994 - acc: 0.961 - ETA: 6:51 - loss: 0.0994 - acc: 0.961 - ETA: 6:49 - loss: 0.0995 - acc: 0.961 - ETA: 6:48 - loss: 0.0995 - acc: 0.961 - ETA: 6:47 - loss: 0.0994 - acc: 0.961 - ETA: 6:46 - loss: 0.0994 - acc: 0.961 - ETA: 6:45 - loss: 0.0994 - acc: 0.961 - ETA: 6:44 - loss: 0.0994 - acc: 0.961 - ETA: 6:43 - loss: 0.0994 - acc: 0.961 - ETA: 6:42 - loss: 0.0995 - acc: 0.961 - ETA: 6:41 - loss: 0.0994 - acc: 0.961 - ETA: 6:40 - loss: 0.0995 - acc: 0.961 - ETA: 6:39 - loss: 0.0995 - acc: 0.961 - ETA: 6:38 - loss: 0.0995 - acc: 0.961 - ETA: 6:37 - loss: 0.0995 - acc: 0.961 - ETA: 6:36 - loss: 0.0995 - acc: 0.961 - ETA: 6:34 - loss: 0.0996 - acc: 0.961 - ETA: 6:33 - loss: 0.0995 - acc: 0.961 - ETA: 6:32 - loss: 0.0995 - acc: 0.961 - ETA: 6:31 - loss: 0.0995 - acc: 0.961 - ETA: 6:30 - loss: 0.0995 - acc: 0.961 - ETA: 6:29 - loss: 0.0995 - acc: 0.961 - ETA: 6:28 - loss: 0.0994 - acc: 0.961 - ETA: 6:27 - loss: 0.0994 - acc: 0.961 - ETA: 6:26 - loss: 0.0994 - acc: 0.961 - ETA: 6:25 - loss: 0.0994 - acc: 0.961 - ETA: 6:24 - loss: 0.0995 - acc: 0.961 - ETA: 6:22 - loss: 0.0994 - acc: 0.961 - ETA: 6:21 - loss: 0.0995 - acc: 0.961 - ETA: 6:20 - loss: 0.0995 - acc: 0.961 - ETA: 6:19 - loss: 0.0994 - acc: 0.961 - ETA: 6:18 - loss: 0.0994 - acc: 0.961 - ETA: 6:17 - loss: 0.0994 - acc: 0.961 - ETA: 6:16 - loss: 0.0994 - acc: 0.961 - ETA: 6:15 - loss: 0.0994 - acc: 0.961 - ETA: 6:14 - loss: 0.0994 - acc: 0.961 - ETA: 6:13 - loss: 0.0993 - acc: 0.961 - ETA: 6:12 - loss: 0.0993 - acc: 0.961 - ETA: 6:10 - loss: 0.0992 - acc: 0.961 - ETA: 6:09 - loss: 0.0993 - acc: 0.961 - ETA: 6:08 - loss: 0.0993 - acc: 0.961 - ETA: 6:07 - loss: 0.0993 - acc: 0.961 - ETA: 6:06 - loss: 0.0993 - acc: 0.961 - ETA: 6:05 - loss: 0.0993 - acc: 0.961 - ETA: 6:04 - loss: 0.0994 - acc: 0.961 - ETA: 6:03 - loss: 0.0994 - acc: 0.961 - ETA: 6:04 - loss: 0.0995 - acc: 0.961 - ETA: 2:26:55 - loss: 0.0994 - acc: 0.96 - ETA: 2:26:06 - loss: 0.0994 - acc: 0.96 - ETA: 2:25:19 - loss: 0.0994 - acc: 0.96 - ETA: 2:24:32 - loss: 0.0994 - acc: 0.96 - ETA: 2:23:44 - loss: 0.0994 - acc: 0.96 - ETA: 2:22:57 - loss: 0.0994 - acc: 0.96 - ETA: 2:22:09 - loss: 0.0994 - acc: 0.96 - ETA: 2:21:22 - loss: 0.0994 - acc: 0.96 - ETA: 2:20:34 - loss: 0.0994 - acc: 0.96 - ETA: 9:32:56 - loss: 0.0994 - acc: 0.96 - ETA: 9:29:38 - loss: 0.0995 - acc: 0.96 - ETA: 9:26:21 - loss: 0.0994 - acc: 0.96 - ETA: 9:23:05 - loss: 0.0995 - acc: 0.9614307712/360490 [========================>.....] - ETA: 9:19:50 - loss: 0.0995 - acc: 0.96 - ETA: 9:16:36 - loss: 0.0995 - acc: 0.96 - ETA: 9:13:23 - loss: 0.0996 - acc: 0.96 - ETA: 9:10:10 - loss: 0.0996 - acc: 0.96 - ETA: 9:06:59 - loss: 0.0996 - acc: 0.96 - ETA: 9:03:49 - loss: 0.0996 - acc: 0.96 - ETA: 9:00:39 - loss: 0.0995 - acc: 0.96 - ETA: 8:57:30 - loss: 0.0995 - acc: 0.96 - ETA: 8:54:22 - loss: 0.0995 - acc: 0.96 - ETA: 8:51:15 - loss: 0.0995 - acc: 0.96 - ETA: 8:48:11 - loss: 0.0995 - acc: 0.96 - ETA: 8:45:06 - loss: 0.0995 - acc: 0.96 - ETA: 8:42:01 - loss: 0.0995 - acc: 0.96 - ETA: 8:38:58 - loss: 0.0995 - acc: 0.96 - ETA: 8:35:55 - loss: 0.0995 - acc: 0.96 - ETA: 8:32:54 - loss: 0.0995 - acc: 0.96 - ETA: 8:29:53 - loss: 0.0994 - acc: 0.96 - ETA: 8:26:53 - loss: 0.0994 - acc: 0.96 - ETA: 8:23:54 - loss: 0.0994 - acc: 0.96 - ETA: 8:20:55 - loss: 0.0994 - acc: 0.96 - ETA: 8:17:58 - loss: 0.0994 - acc: 0.96 - ETA: 8:15:01 - loss: 0.0994 - acc: 0.96 - ETA: 8:12:05 - loss: 0.0994 - acc: 0.96 - ETA: 8:09:10 - loss: 0.0994 - acc: 0.96 - ETA: 8:06:16 - loss: 0.0994 - acc: 0.96 - ETA: 8:03:22 - loss: 0.0994 - acc: 0.96 - ETA: 8:00:30 - loss: 0.0995 - acc: 0.96 - ETA: 7:57:38 - loss: 0.0995 - acc: 0.96 - ETA: 7:54:46 - loss: 0.0995 - acc: 0.96 - ETA: 7:51:56 - loss: 0.0995 - acc: 0.96 - ETA: 7:49:06 - loss: 0.0995 - acc: 0.96 - ETA: 7:46:17 - loss: 0.0995 - acc: 0.96 - ETA: 7:43:29 - loss: 0.0995 - acc: 0.96 - ETA: 7:40:42 - loss: 0.0995 - acc: 0.96 - ETA: 7:37:56 - loss: 0.0994 - acc: 0.96 - ETA: 7:35:10 - loss: 0.0994 - acc: 0.96 - ETA: 7:32:25 - loss: 0.0994 - acc: 0.96 - ETA: 7:29:40 - loss: 0.0994 - acc: 0.96 - ETA: 7:26:57 - loss: 0.0994 - acc: 0.96 - ETA: 7:24:14 - loss: 0.0995 - acc: 0.96 - ETA: 7:21:31 - loss: 0.0995 - acc: 0.96 - ETA: 7:18:50 - loss: 0.0994 - acc: 0.96 - ETA: 7:16:09 - loss: 0.0995 - acc: 0.96 - ETA: 7:13:29 - loss: 0.0995 - acc: 0.96 - ETA: 7:10:50 - loss: 0.0994 - acc: 0.96 - ETA: 7:08:11 - loss: 0.0994 - acc: 0.96 - ETA: 7:05:33 - loss: 0.0995 - acc: 0.96 - ETA: 7:02:56 - loss: 0.0996 - acc: 0.96 - ETA: 7:00:19 - loss: 0.0996 - acc: 0.96 - ETA: 6:57:43 - loss: 0.0996 - acc: 0.96 - ETA: 6:55:08 - loss: 0.0996 - acc: 0.96 - ETA: 6:52:33 - loss: 0.0996 - acc: 0.96 - ETA: 6:49:59 - loss: 0.0997 - acc: 0.96 - ETA: 6:47:26 - loss: 0.0996 - acc: 0.96 - ETA: 6:44:53 - loss: 0.0997 - acc: 0.96 - ETA: 6:42:22 - loss: 0.0996 - acc: 0.96 - ETA: 6:39:50 - loss: 0.0996 - acc: 0.96 - ETA: 6:37:20 - loss: 0.0996 - acc: 0.96 - ETA: 6:34:50 - loss: 0.0996 - acc: 0.96 - ETA: 6:32:20 - loss: 0.0995 - acc: 0.96 - ETA: 6:29:51 - loss: 0.0996 - acc: 0.96 - ETA: 6:27:23 - loss: 0.0997 - acc: 0.96 - ETA: 6:24:56 - loss: 0.0997 - acc: 0.96 - ETA: 6:22:29 - loss: 0.0996 - acc: 0.96 - ETA: 6:20:03 - loss: 0.0996 - acc: 0.96 - ETA: 6:17:37 - loss: 0.0996 - acc: 0.96 - ETA: 6:15:12 - loss: 0.0996 - acc: 0.96 - ETA: 6:12:48 - loss: 0.0996 - acc: 0.96 - ETA: 6:10:24 - loss: 0.0996 - acc: 0.96 - ETA: 6:08:01 - loss: 0.0996 - acc: 0.96 - ETA: 6:05:38 - loss: 0.0997 - acc: 0.96 - ETA: 6:03:16 - loss: 0.0997 - acc: 0.96 - ETA: 6:00:54 - loss: 0.0996 - acc: 0.96 - ETA: 5:58:34 - loss: 0.0996 - acc: 0.96 - ETA: 5:56:13 - loss: 0.0996 - acc: 0.96 - ETA: 5:53:53 - loss: 0.0996 - acc: 0.96 - ETA: 5:51:34 - loss: 0.0996 - acc: 0.96 - ETA: 5:49:16 - loss: 0.0996 - acc: 0.96 - ETA: 5:46:58 - loss: 0.0995 - acc: 0.96 - ETA: 5:44:40 - loss: 0.0996 - acc: 0.96 - ETA: 5:42:23 - loss: 0.0996 - acc: 0.96 - ETA: 5:40:07 - loss: 0.0996 - acc: 0.96 - ETA: 5:37:51 - loss: 0.0997 - acc: 0.96 - ETA: 5:35:36 - loss: 0.0996 - acc: 0.96 - ETA: 5:33:21 - loss: 0.0996 - acc: 0.96 - ETA: 5:31:07 - loss: 0.0996 - acc: 0.96 - ETA: 5:28:53 - loss: 0.0996 - acc: 0.96 - ETA: 5:26:40 - loss: 0.0996 - acc: 0.96 - ETA: 5:24:28 - loss: 0.0996 - acc: 0.96 - ETA: 5:22:16 - loss: 0.0996 - acc: 0.96 - ETA: 5:20:04 - loss: 0.0996 - acc: 0.96 - ETA: 5:17:53 - loss: 0.0996 - acc: 0.96 - ETA: 5:15:43 - loss: 0.0995 - acc: 0.96 - ETA: 5:13:33 - loss: 0.0996 - acc: 0.96 - ETA: 5:11:24 - loss: 0.0996 - acc: 0.96 - ETA: 5:09:15 - loss: 0.0996 - acc: 0.96 - ETA: 5:07:06 - loss: 0.0996 - acc: 0.96 - ETA: 5:04:58 - loss: 0.0996 - acc: 0.96 - ETA: 5:02:51 - loss: 0.0996 - acc: 0.96 - ETA: 5:00:44 - loss: 0.0995 - acc: 0.96 - ETA: 4:58:38 - loss: 0.0996 - acc: 0.96 - ETA: 4:56:32 - loss: 0.0996 - acc: 0.96 - ETA: 4:54:26 - loss: 0.0996 - acc: 0.96 - ETA: 4:52:21 - loss: 0.0996 - acc: 0.96 - ETA: 4:50:17 - loss: 0.0996 - acc: 0.96 - ETA: 4:48:13 - loss: 0.0996 - acc: 0.96 - ETA: 4:46:09 - loss: 0.0996 - acc: 0.96 - ETA: 4:44:06 - loss: 0.0995 - acc: 0.96 - ETA: 4:42:04 - loss: 0.0995 - acc: 0.96 - ETA: 4:40:01 - loss: 0.0994 - acc: 0.96 - ETA: 4:38:00 - loss: 0.0994 - acc: 0.96 - ETA: 4:35:59 - loss: 0.0994 - acc: 0.96 - ETA: 4:33:58 - loss: 0.0993 - acc: 0.96 - ETA: 4:31:58 - loss: 0.0993 - acc: 0.96 - ETA: 4:29:58 - loss: 0.0993 - acc: 0.96 - ETA: 4:27:58 - loss: 0.0993 - acc: 0.96 - ETA: 4:26:00 - loss: 0.0993 - acc: 0.96 - ETA: 4:24:01 - loss: 0.0992 - acc: 0.96 - ETA: 4:22:03 - loss: 0.0992 - acc: 0.96 - ETA: 4:20:06 - loss: 0.0993 - acc: 0.96 - ETA: 4:18:09 - loss: 0.0993 - acc: 0.96 - ETA: 4:16:12 - loss: 0.0993 - acc: 0.96 - ETA: 4:14:16 - loss: 0.0993 - acc: 0.96 - ETA: 4:12:20 - loss: 0.0993 - acc: 0.96 - ETA: 4:10:24 - loss: 0.0992 - acc: 0.96 - ETA: 4:08:29 - loss: 0.0992 - acc: 0.96 - ETA: 4:06:35 - loss: 0.0993 - acc: 0.96 - ETA: 4:04:41 - loss: 0.0993 - acc: 0.96 - ETA: 4:02:47 - loss: 0.0993 - acc: 0.96 - ETA: 4:00:54 - loss: 0.0992 - acc: 0.96 - ETA: 3:59:01 - loss: 0.0992 - acc: 0.96 - ETA: 3:57:09 - loss: 0.0992 - acc: 0.96 - ETA: 3:55:17 - loss: 0.0992 - acc: 0.96 - ETA: 3:53:25 - loss: 0.0991 - acc: 0.96 - ETA: 3:51:34 - loss: 0.0991 - acc: 0.96 - ETA: 3:49:43 - loss: 0.0991 - acc: 0.96 - ETA: 3:47:53 - loss: 0.0991 - acc: 0.96 - ETA: 3:46:03 - loss: 0.0990 - acc: 0.96 - ETA: 3:44:13 - loss: 0.0990 - acc: 0.96 - ETA: 3:42:24 - loss: 0.0990 - acc: 0.96 - ETA: 3:40:35 - loss: 0.0990 - acc: 0.96 - ETA: 3:38:47 - loss: 0.0989 - acc: 0.96 - ETA: 3:36:59 - loss: 0.0989 - acc: 0.96 - ETA: 3:35:11 - loss: 0.0990 - acc: 0.96 - ETA: 3:33:24 - loss: 0.0989 - acc: 0.96 - ETA: 3:31:37 - loss: 0.0990 - acc: 0.96 - ETA: 3:29:51 - loss: 0.0989 - acc: 0.96 - ETA: 3:28:04 - loss: 0.0989 - acc: 0.96 - ETA: 3:26:19 - loss: 0.0990 - acc: 0.96 - ETA: 3:24:33 - loss: 0.0990 - acc: 0.96 - ETA: 3:22:48 - loss: 0.0990 - acc: 0.96 - ETA: 3:21:04 - loss: 0.0990 - acc: 0.96 - ETA: 3:19:20 - loss: 0.0990 - acc: 0.96 - ETA: 3:17:36 - loss: 0.0989 - acc: 0.96 - ETA: 3:15:52 - loss: 0.0990 - acc: 0.96 - ETA: 3:14:09 - loss: 0.0990 - acc: 0.96 - ETA: 3:12:26 - loss: 0.0990 - acc: 0.96 - ETA: 3:10:44 - loss: 0.0991 - acc: 0.96 - ETA: 3:09:02 - loss: 0.0991 - acc: 0.96 - ETA: 3:07:20 - loss: 0.0991 - acc: 0.96 - ETA: 3:05:39 - loss: 0.0991 - acc: 0.96 - ETA: 3:03:58 - loss: 0.0990 - acc: 0.96 - ETA: 3:02:17 - loss: 0.0990 - acc: 0.96 - ETA: 3:00:37 - loss: 0.0990 - acc: 0.96 - ETA: 2:58:57 - loss: 0.0991 - acc: 0.96 - ETA: 2:57:17 - loss: 0.0990 - acc: 0.96 - ETA: 2:55:38 - loss: 0.0990 - acc: 0.96 - ETA: 2:53:59 - loss: 0.0990 - acc: 0.96 - ETA: 2:52:20 - loss: 0.0990 - acc: 0.96 - ETA: 2:50:42 - loss: 0.0990 - acc: 0.96 - ETA: 2:49:04 - loss: 0.0990 - acc: 0.96 - ETA: 2:47:27 - loss: 0.0989 - acc: 0.96 - ETA: 2:45:49 - loss: 0.0989 - acc: 0.96 - ETA: 2:44:12 - loss: 0.0989 - acc: 0.96 - ETA: 2:42:36 - loss: 0.0989 - acc: 0.96 - ETA: 2:40:59 - loss: 0.0989 - acc: 0.96 - ETA: 2:39:24 - loss: 0.0990 - acc: 0.96 - ETA: 2:37:48 - loss: 0.0990 - acc: 0.96 - ETA: 2:36:13 - loss: 0.0990 - acc: 0.96 - ETA: 2:34:38 - loss: 0.0990 - acc: 0.96 - ETA: 2:33:03 - loss: 0.0990 - acc: 0.96 - ETA: 2:31:29 - loss: 0.0990 - acc: 0.96 - ETA: 2:29:54 - loss: 0.0990 - acc: 0.96 - ETA: 2:28:21 - loss: 0.0990 - acc: 0.96 - ETA: 2:26:47 - loss: 0.0989 - acc: 0.96 - ETA: 2:25:14 - loss: 0.0989 - acc: 0.96 - ETA: 2:23:41 - loss: 0.0989 - acc: 0.96 - ETA: 2:22:09 - loss: 0.0989 - acc: 0.96 - ETA: 2:20:37 - loss: 0.0989 - acc: 0.96 - ETA: 2:19:05 - loss: 0.0989 - acc: 0.96 - ETA: 2:17:33 - loss: 0.0989 - acc: 0.96 - ETA: 2:16:02 - loss: 0.0989 - acc: 0.96 - ETA: 2:14:31 - loss: 0.0989 - acc: 0.96 - ETA: 2:13:01 - loss: 0.0989 - acc: 0.9617"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360490/360490 [==============================] - ETA: 2:11:30 - loss: 0.0989 - acc: 0.96 - ETA: 2:10:00 - loss: 0.0989 - acc: 0.96 - ETA: 2:08:30 - loss: 0.0989 - acc: 0.96 - ETA: 2:07:01 - loss: 0.0989 - acc: 0.96 - ETA: 2:05:32 - loss: 0.0989 - acc: 0.96 - ETA: 2:04:03 - loss: 0.0989 - acc: 0.96 - ETA: 2:02:34 - loss: 0.0988 - acc: 0.96 - ETA: 2:01:06 - loss: 0.0988 - acc: 0.96 - ETA: 1:59:38 - loss: 0.0987 - acc: 0.96 - ETA: 1:58:10 - loss: 0.0987 - acc: 0.96 - ETA: 1:56:43 - loss: 0.0987 - acc: 0.96 - ETA: 1:55:15 - loss: 0.0987 - acc: 0.96 - ETA: 1:53:48 - loss: 0.0987 - acc: 0.96 - ETA: 1:52:22 - loss: 0.0987 - acc: 0.96 - ETA: 1:50:55 - loss: 0.0986 - acc: 0.96 - ETA: 1:49:29 - loss: 0.0986 - acc: 0.96 - ETA: 1:48:04 - loss: 0.0986 - acc: 0.96 - ETA: 1:46:38 - loss: 0.0986 - acc: 0.96 - ETA: 1:45:13 - loss: 0.0986 - acc: 0.96 - ETA: 1:43:48 - loss: 0.0986 - acc: 0.96 - ETA: 1:42:23 - loss: 0.0986 - acc: 0.96 - ETA: 1:40:59 - loss: 0.0986 - acc: 0.96 - ETA: 1:39:34 - loss: 0.0986 - acc: 0.96 - ETA: 1:38:11 - loss: 0.0986 - acc: 0.96 - ETA: 1:36:47 - loss: 0.0985 - acc: 0.96 - ETA: 1:35:24 - loss: 0.0985 - acc: 0.96 - ETA: 1:34:00 - loss: 0.0986 - acc: 0.96 - ETA: 1:32:38 - loss: 0.0985 - acc: 0.96 - ETA: 1:31:15 - loss: 0.0986 - acc: 0.96 - ETA: 1:29:53 - loss: 0.0986 - acc: 0.96 - ETA: 1:28:30 - loss: 0.0986 - acc: 0.96 - ETA: 1:27:09 - loss: 0.0986 - acc: 0.96 - ETA: 1:25:47 - loss: 0.0986 - acc: 0.96 - ETA: 1:24:26 - loss: 0.0986 - acc: 0.96 - ETA: 1:23:05 - loss: 0.0986 - acc: 0.96 - ETA: 1:21:44 - loss: 0.0986 - acc: 0.96 - ETA: 1:20:23 - loss: 0.0986 - acc: 0.96 - ETA: 1:19:03 - loss: 0.0986 - acc: 0.96 - ETA: 1:17:43 - loss: 0.0986 - acc: 0.96 - ETA: 1:16:23 - loss: 0.0986 - acc: 0.96 - ETA: 1:15:04 - loss: 0.0986 - acc: 0.96 - ETA: 1:13:44 - loss: 0.0986 - acc: 0.96 - ETA: 1:12:25 - loss: 0.0986 - acc: 0.96 - ETA: 1:11:06 - loss: 0.0986 - acc: 0.96 - ETA: 1:09:48 - loss: 0.0986 - acc: 0.96 - ETA: 1:08:29 - loss: 0.0986 - acc: 0.96 - ETA: 1:07:11 - loss: 0.0986 - acc: 0.96 - ETA: 1:05:53 - loss: 0.0986 - acc: 0.96 - ETA: 1:04:36 - loss: 0.0986 - acc: 0.96 - ETA: 1:03:18 - loss: 0.0986 - acc: 0.96 - ETA: 1:02:01 - loss: 0.0986 - acc: 0.96 - ETA: 1:00:44 - loss: 0.0986 - acc: 0.96 - ETA: 59:27 - loss: 0.0986 - acc: 0.9617 - ETA: 58:11 - loss: 0.0986 - acc: 0.96 - ETA: 56:55 - loss: 0.0986 - acc: 0.96 - ETA: 55:39 - loss: 0.0986 - acc: 0.96 - ETA: 54:23 - loss: 0.0986 - acc: 0.96 - ETA: 53:07 - loss: 0.0986 - acc: 0.96 - ETA: 51:52 - loss: 0.0986 - acc: 0.96 - ETA: 50:37 - loss: 0.0985 - acc: 0.96 - ETA: 49:22 - loss: 0.0985 - acc: 0.96 - ETA: 48:07 - loss: 0.0985 - acc: 0.96 - ETA: 46:53 - loss: 0.0984 - acc: 0.96 - ETA: 45:38 - loss: 0.0984 - acc: 0.96 - ETA: 44:24 - loss: 0.0984 - acc: 0.96 - ETA: 43:11 - loss: 0.0983 - acc: 0.96 - ETA: 41:57 - loss: 0.0983 - acc: 0.96 - ETA: 40:44 - loss: 0.0983 - acc: 0.96 - ETA: 39:30 - loss: 0.0983 - acc: 0.96 - ETA: 38:17 - loss: 0.0983 - acc: 0.96 - ETA: 37:05 - loss: 0.0984 - acc: 0.96 - ETA: 35:52 - loss: 0.0984 - acc: 0.96 - ETA: 34:40 - loss: 0.0984 - acc: 0.96 - ETA: 33:28 - loss: 0.0984 - acc: 0.96 - ETA: 32:16 - loss: 0.0984 - acc: 0.96 - ETA: 31:04 - loss: 0.0984 - acc: 0.96 - ETA: 29:53 - loss: 0.0983 - acc: 0.96 - ETA: 28:42 - loss: 0.0983 - acc: 0.96 - ETA: 27:30 - loss: 0.0983 - acc: 0.96 - ETA: 26:20 - loss: 0.0982 - acc: 0.96 - ETA: 25:09 - loss: 0.0982 - acc: 0.96 - ETA: 23:59 - loss: 0.0982 - acc: 0.96 - ETA: 22:48 - loss: 0.0982 - acc: 0.96 - ETA: 21:38 - loss: 0.0982 - acc: 0.96 - ETA: 20:28 - loss: 0.0982 - acc: 0.96 - ETA: 19:19 - loss: 0.0982 - acc: 0.96 - ETA: 18:09 - loss: 0.0982 - acc: 0.96 - ETA: 17:00 - loss: 0.0982 - acc: 0.96 - ETA: 15:51 - loss: 0.0981 - acc: 0.96 - ETA: 14:42 - loss: 0.0981 - acc: 0.96 - ETA: 13:34 - loss: 0.0981 - acc: 0.96 - ETA: 12:25 - loss: 0.0981 - acc: 0.96 - ETA: 11:17 - loss: 0.0981 - acc: 0.96 - ETA: 10:09 - loss: 0.0982 - acc: 0.96 - ETA: 9:01 - loss: 0.0982 - acc: 0.9618 - ETA: 7:53 - loss: 0.0982 - acc: 0.961 - ETA: 6:46 - loss: 0.0982 - acc: 0.961 - ETA: 5:39 - loss: 0.0981 - acc: 0.961 - ETA: 4:31 - loss: 0.0981 - acc: 0.961 - ETA: 3:25 - loss: 0.0981 - acc: 0.961 - ETA: 2:18 - loss: 0.0981 - acc: 0.961 - ETA: 1:11 - loss: 0.0982 - acc: 0.961 - ETA: 5s - loss: 0.0981 - acc: 0.9618  - 46663s 129ms/step - loss: 0.0981 - acc: 0.9618 - val_loss: 0.1235 - val_acc: 0.9530\n",
      "Val F1 Score: 0.6239\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-0b9b458d051c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Val F1 Score: {:.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpred_test_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "model = model_cnn(embedding_matrix1)\n",
    "\n",
    "for e in range(2):\n",
    "    model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n",
    "    pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "\n",
    "    best_thresh = 0.5\n",
    "    best_score = 0.0\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        score = f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "        if score > best_score:\n",
    "            best_thresh = thresh\n",
    "            best_score = score\n",
    "\n",
    "    print(\"Val F1 Score: {:.4f}\".format(best_score))\n",
    "\n",
    "# pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_val_y = model2.predict([val_X], batch_size=1024, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = [each[0] for each in (pred_val_y > 0.19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df[mask].to_csv('val_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00393321],\n",
       "       [ 0.00129482],\n",
       "       [ 0.00010186],\n",
       "       ..., \n",
       "       [ 0.0008012 ],\n",
       "       [ 0.00826971],\n",
       "       [ 0.01254148]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# model.save('my_model2.h5', 'w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = load_model('my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = model2.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = [each[0] for each in (res > 0.19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56370, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56370, 70)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best threshold for cutting the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working function for prediction, it is not fast, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_label(query='What is happiness?', maxlen = 70, best_thresh=0.19):\n",
    "    from keras.models import load_model\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    import pickle\n",
    "    import time\n",
    "    start = time.time()\n",
    "    # loading\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    test_X = tokenizer.texts_to_sequences([query])\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    model = load_model('my_model2.h5')\n",
    "    score = model.predict(test_X)\n",
    "    print('took {:.2f} seconds to finish'.format(time.time() - start))\n",
    "    print(score)\n",
    "    if score[0] > best_thresh:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 6.27 seconds to finish\n",
      "[[ 0.00302711]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label('Which races have the smallest penis?\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## highlight words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = list(positive.question_text)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13814"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are 0.06724498987414163\n",
      "to 0.17567827433455765\n",
      "why 0.05241173128141058\n",
      "is 0.0636212093984737\n",
      "do 0.06419208940178942\n",
      "it 0.08115724057937664\n",
      "considered 0.16891314396138082\n",
      "so 0.09124197458508662\n",
      "now 0.14474915181836895\n",
      "people 0.08397287609637323\n",
      "many 0.11790505761367612\n",
      "think 0.1156672194710421\n",
      "good 0.1430831844359977\n",
      "old 0.14881925841170807\n",
      "health 0.19960560754904966\n",
      "year 0.3061701228726695\n",
      "26 0.49483210960155244\n",
      "olds 0.254549686239098\n",
      "officially 0.2525913039829496\n",
      "idea 0.1904335041601847\n",
      "teenagers 0.22978386340895543\n",
      "allow 0.17707908131054775\n",
      "insurance 0.23538596701197168\n",
      "stick 0.22774087517639086\n",
      "mommy 0.27860047670037996\n"
     ]
    }
   ],
   "source": [
    "doc = randint(0, 80810)\n",
    "feature_index = X[doc,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [X[doc, x] for x in feature_index])\n",
    "\n",
    "for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "    print(w, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = list(train_df.question_text)\n",
    "cvectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "XL = cvectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression(C=1.0)\n",
    "lg.fit(XL, list(train_df.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3195271"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lg.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = XL[397,:].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_scores = list(zip(features, lg.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do all Muslims think non-Muslims go to hell?'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.sample(1).iloc[0]['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Has the United States become the largest dictatorship in the world?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[22]['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', -0.05765709028742363),\n",
       " ('do moms', 0.4288182345969702),\n",
       " ('have', 0.03596763207339017),\n",
       " ('have sex', 0.38521374112419804),\n",
       " ('moms', 1.0969324090070018),\n",
       " ('moms have', 0.3057235643949789),\n",
       " ('sex', 1.009243853334876),\n",
       " ('sex with', 1.6025979871460407),\n",
       " ('sons', 0.6369177775802709),\n",
       " ('their', 0.40886481490242),\n",
       " ('their sons', 0.6909477098510638),\n",
       " ('with', 0.005402496547504425),\n",
       " ('with their', -0.008730347028346705)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lg_scores[x] for x in feature_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = dict(lg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('lg_score.pickle', 'wb') as handle:\n",
    "    pickle.dump(score_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('count_vectorizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(cvectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lg_coef.pickle', 'wb') as handle:\n",
    "    pickle.dump(lg.coef_, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('count_vectorizer.pickle', 'rb') as handle:\n",
    "    cv = pickle.load(handle)\n",
    "with open('lg_coef.pickle', 'rb') as handle:\n",
    "    lg_coef = pickle.load(handle)\n",
    "    \n",
    "transformed_query = cv.transform(['Has the United States become the largest dictatorship in the world?'])\n",
    "indices = transformed_query.nonzero()[1]\n",
    "coef_list = lg_coef[0][indices]\n",
    "largest_n = coef_list.argsort()[-3:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['become', 'become the', 'dictatorship', 'dictatorship in', 'has',\n",
       "       'has the', 'in', 'in the', 'largest', 'largest dictatorship',\n",
       "       'states', 'states become', 'the', 'the largest', 'the united',\n",
       "       'the world', 'united', 'united states', 'world'], dtype='<U236')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cv.get_feature_names())[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_words(query='Has the United States become the largest dictatorship in the world?'):\n",
    "    # preprocessing\n",
    "    from nltk import word_tokenize\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    bigram = [tokens[ii]+' '+tokens[ii+1] for ii in range()]\n",
    "    \n",
    "    with open('../data/lg_score1.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "        \n",
    "    scores = [b[t] if t in b else 0 for t in tokens]\n",
    "\n",
    "#     import numpy as np\n",
    "#     arr = np.array(scores)\n",
    "#     indices = arr.argsort()[-5:][::-1]\n",
    "    print(scores)\n",
    "\n",
    "    indices = [scores.index(ii) for ii in scores if ii >= 1.0]\n",
    "    \n",
    "    words = list(set([tokens[i] for i in indices]))\n",
    "    print(words)\n",
    "    ans = []\n",
    "    for ind, ii in enumerate(query.lower().split()):\n",
    "        for jj in words:\n",
    "            if jj in ii and len(ii) - len(jj) <= 1:\n",
    "                ans.append(ind)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.516075785049279, -0.05765709028742363, 0, 0.2537544305507007, 0.5821064684668563, 0.43686076674634394, 0.10958945321485326, 1.7259926004064412, -0.016849850760210776, 0.3445836623080487, -0.18207464419112712, -0.8467612428737705, 1.8839439282359793, -0.34902953204465775, 0.12487766041081051, 0, 0, 0, 0.050726782377775885, 0.2005054389120148, 0.3349759906450282, -0.1460054169221432, -0.04507660825735099, -0.43625239740076927, 0]\n",
      "['terrorists', 'trump']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 11]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_words(\"Why don't USA citizens realize that Trump is rapidly doing what terrorists could not, i.e., push the country towards irrevocable catastrophe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_words(\"Why don't poor countries print more money to use for paying for education, etc.?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
